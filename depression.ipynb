{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (27901, 18)\n",
      "\n",
      "Data Types:\n",
      "id                                         int64\n",
      "Gender                                    object\n",
      "Age                                      float64\n",
      "City                                      object\n",
      "Profession                                object\n",
      "Academic Pressure                        float64\n",
      "Work Pressure                            float64\n",
      "CGPA                                     float64\n",
      "Study Satisfaction                       float64\n",
      "Job Satisfaction                         float64\n",
      "Sleep Duration                            object\n",
      "Dietary Habits                            object\n",
      "Degree                                    object\n",
      "Have you ever had suicidal thoughts ?     object\n",
      "Work/Study Hours                         float64\n",
      "Financial Stress                          object\n",
      "Family History of Mental Illness          object\n",
      "Depression                                 int64\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "id                                       0\n",
      "Gender                                   0\n",
      "Age                                      0\n",
      "City                                     0\n",
      "Profession                               0\n",
      "Academic Pressure                        0\n",
      "Work Pressure                            0\n",
      "CGPA                                     0\n",
      "Study Satisfaction                       0\n",
      "Job Satisfaction                         0\n",
      "Sleep Duration                           0\n",
      "Dietary Habits                           0\n",
      "Degree                                   0\n",
      "Have you ever had suicidal thoughts ?    0\n",
      "Work/Study Hours                         0\n",
      "Financial Stress                         0\n",
      "Family History of Mental Illness         0\n",
      "Depression                               0\n",
      "dtype: int64\n",
      "\n",
      "Numeric columns: ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours', 'Depression']\n",
      "\n",
      "Categorical columns: ['Gender', 'City', 'Profession', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Financial Stress', 'Family History of Mental Illness']\n",
      "\n",
      "Variable Types:\n",
      "Gender: Binary\n",
      "Age: Numeric (Continuous)\n",
      "City: Nominal\n",
      "Profession: Nominal\n",
      "Academic Pressure: Ordinal\n",
      "Work Pressure: Ordinal\n",
      "CGPA: Numeric (Continuous)\n",
      "Study Satisfaction: Ordinal\n",
      "Job Satisfaction: Ordinal\n",
      "Sleep Duration: Nominal\n",
      "Dietary Habits: Nominal\n",
      "Degree: Nominal\n",
      "Have you ever had suicidal thoughts ?: Binary\n",
      "Work/Study Hours: Numeric (Continuous)\n",
      "Financial Stress: Nominal\n",
      "Family History of Mental Illness: Binary\n",
      "Depression: Binary\n",
      "\n",
      "Statistics for Age:\n",
      "Mean: 25.82230027597577\n",
      "Median: 25.0\n",
      "Mode: 24.0\n",
      "Midrange: 38.5\n",
      "Five Number Summary:\n",
      "  Minimum: 18.0\n",
      "  Q1: 21.0\n",
      "  Median: 25.0\n",
      "  Q3: 30.0\n",
      "  Maximum: 59.0\n",
      "\n",
      "Statistics for Academic Pressure:\n",
      "Mean: 3.1412135765743163\n",
      "Median: 3.0\n",
      "Mode: 3.0\n",
      "Midrange: 2.5\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 2.0\n",
      "  Median: 3.0\n",
      "  Q3: 4.0\n",
      "  Maximum: 5.0\n",
      "\n",
      "Statistics for Work Pressure:\n",
      "Mean: 0.00043009211139385684\n",
      "Median: 0.0\n",
      "Mode: 0.0\n",
      "Midrange: 2.5\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 0.0\n",
      "  Median: 0.0\n",
      "  Q3: 0.0\n",
      "  Maximum: 5.0\n",
      "\n",
      "Statistics for CGPA:\n",
      "Mean: 7.65610417189348\n",
      "Median: 7.77\n",
      "Mode: 8.04\n",
      "Midrange: 5.0\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 6.29\n",
      "  Median: 7.77\n",
      "  Q3: 8.92\n",
      "  Maximum: 10.0\n",
      "\n",
      "Statistics for Study Satisfaction:\n",
      "Mean: 2.943837138453819\n",
      "Median: 3.0\n",
      "Mode: 4.0\n",
      "Midrange: 2.5\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 2.0\n",
      "  Median: 3.0\n",
      "  Q3: 4.0\n",
      "  Maximum: 5.0\n",
      "\n",
      "Statistics for Job Satisfaction:\n",
      "Mean: 0.0006809791763736067\n",
      "Median: 0.0\n",
      "Mode: 0.0\n",
      "Midrange: 2.0\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 0.0\n",
      "  Median: 0.0\n",
      "  Q3: 0.0\n",
      "  Maximum: 4.0\n",
      "\n",
      "Statistics for Work/Study Hours:\n",
      "Mean: 7.156983620658758\n",
      "Median: 8.0\n",
      "Mode: 10.0\n",
      "Midrange: 6.0\n",
      "Five Number Summary:\n",
      "  Minimum: 0.0\n",
      "  Q1: 4.0\n",
      "  Median: 8.0\n",
      "  Q3: 10.0\n",
      "  Maximum: 12.0\n",
      "\n",
      "Statistics for Depression:\n",
      "Mean: 0.5854987276441704\n",
      "Median: 1.0\n",
      "Mode: 1\n",
      "Midrange: 0.5\n",
      "Five Number Summary:\n",
      "  Minimum: 0\n",
      "  Q1: 0.0\n",
      "  Median: 1.0\n",
      "  Q3: 1.0\n",
      "  Maximum: 1\n",
      "1. Created histogram for Age\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\_base.py:949: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\_base.py:949: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\_base.py:949: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\_base.py:949: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\_base.py:949: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Created box plot for numeric variables\n",
      "3. Created QQ Plot for Age\n",
      "4. Created correlation heatmap\n",
      "5. Created scatter plot between Age and Academic Pressure\n",
      "6. Created bar chart for Gender\n",
      "7. Created pie chart for Gender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\_base.py:949: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\_base.py:949: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_subset = grouped_data.get_group(pd_key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Created violin plot for Age by Gender\n",
      "9. Created quantile plot for Age\n",
      "10. Created pair plot for numeric variables\n",
      "\n",
      "Analysis complete. All plots have been saved to the 'plots' directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Create output directory for plots\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('student_depression_dataset.csv')\n",
    "\n",
    "# Basic info and statistical summary\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Identify variable types\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if 'id' in numeric_cols:\n",
    "    numeric_cols.remove('id')  # Removing ID as per instructions\n",
    "    \n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric columns:\", numeric_cols)\n",
    "print(\"\\nCategorical columns:\", categorical_cols)\n",
    "\n",
    "# Function to identify data type\n",
    "def identify_variable_type(col):\n",
    "    if df[col].dtype == 'object':\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values == 2:\n",
    "            return \"Binary\"\n",
    "        elif unique_values <= 10:\n",
    "            return \"Nominal\" if not all(df[col].dropna().astype(str).str.isnumeric()) else \"Ordinal\"\n",
    "        else:\n",
    "            return \"Nominal\"\n",
    "    else:  # numeric\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values == 2:\n",
    "            return \"Binary\"\n",
    "        elif unique_values <= 10:\n",
    "            return \"Ordinal\" if col not in ['id'] else \"ID\"\n",
    "        else:\n",
    "            return \"Numeric (Continuous)\"\n",
    "\n",
    "# Identify variable types\n",
    "print(\"\\nVariable Types:\")\n",
    "for col in df.columns:\n",
    "    if col != 'id':  # Skip id column\n",
    "        print(f\"{col}: {identify_variable_type(col)}\")\n",
    "\n",
    "# Calculate statistics for numeric variables\n",
    "def calculate_statistics(df, column):\n",
    "    if column in df.columns and column != 'id':\n",
    "        data = df[column].dropna()\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(data):\n",
    "            # Calculate statistics\n",
    "            mean = data.mean()\n",
    "            median = data.median()\n",
    "            mode = data.mode()[0]\n",
    "            midrange = (data.max() + data.min()) / 2\n",
    "            q1 = data.quantile(0.25)\n",
    "            q3 = data.quantile(0.75)\n",
    "            min_val = data.min()\n",
    "            max_val = data.max()\n",
    "            \n",
    "            print(f\"\\nStatistics for {column}:\")\n",
    "            print(f\"Mean: {mean}\")\n",
    "            print(f\"Median: {median}\")\n",
    "            print(f\"Mode: {mode}\")\n",
    "            print(f\"Midrange: {midrange}\")\n",
    "            print(f\"Five Number Summary:\")\n",
    "            print(f\"  Minimum: {min_val}\")\n",
    "            print(f\"  Q1: {q1}\")\n",
    "            print(f\"  Median: {median}\")\n",
    "            print(f\"  Q3: {q3}\")\n",
    "            print(f\"  Maximum: {max_val}\")\n",
    "            \n",
    "            return {\n",
    "                'mean': mean,\n",
    "                'median': median,\n",
    "                'mode': mode,\n",
    "                'midrange': midrange,\n",
    "                'min': min_val,\n",
    "                'q1': q1,\n",
    "                'q3': q3,\n",
    "                'max': max_val\n",
    "            }\n",
    "        else:\n",
    "            print(f\"\\n{column} is not numeric, skipping statistics.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"\\n{column} not found in dataset or is ID column.\")\n",
    "        return None\n",
    "\n",
    "# Create the 10 required plots\n",
    "def create_plots(df):\n",
    "    # 1. Histogram for a numeric column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    numeric_col = numeric_cols[0] if numeric_cols else None\n",
    "    if numeric_col:\n",
    "        sns.histplot(df[numeric_col], kde=True)\n",
    "        plt.title(f'Histogram of {numeric_col}')\n",
    "        plt.xlabel(numeric_col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/1_histogram_{numeric_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"1. Created histogram for {numeric_col}\")\n",
    "    \n",
    "    # 2. Box Plot for numeric columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(data=df[numeric_cols[:5]])  # Limit to first 5 numeric columns\n",
    "        plt.title('Box Plot of Numeric Variables')\n",
    "        plt.ylabel('Value')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/2_boxplot.png')\n",
    "        plt.close()\n",
    "        print(\"2. Created box plot for numeric variables\")\n",
    "    \n",
    "    # 3. QQ Plot for a numeric column\n",
    "    if numeric_col:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        stats.probplot(df[numeric_col].dropna(), dist=\"norm\", plot=plt)\n",
    "        plt.title(f'Q-Q Plot of {numeric_col}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/3_qqplot_{numeric_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"3. Created QQ Plot for {numeric_col}\")\n",
    "    \n",
    "    # 4. Correlation Heatmap for numeric columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr = df[numeric_cols].corr()\n",
    "        mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "        sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "        plt.title('Correlation Heatmap of Numeric Variables')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/4_correlation_heatmap.png')\n",
    "        plt.close()\n",
    "        print(\"4. Created correlation heatmap\")\n",
    "    \n",
    "    # 5. Scatter Plot between two numeric columns\n",
    "    if len(numeric_cols) >= 2:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=df[numeric_cols[0]], y=df[numeric_cols[1]])\n",
    "        plt.title(f'Scatter Plot: {numeric_cols[0]} vs {numeric_cols[1]}')\n",
    "        plt.xlabel(numeric_cols[0])\n",
    "        plt.ylabel(numeric_cols[1])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/5_scatterplot.png')\n",
    "        plt.close()\n",
    "        print(f\"5. Created scatter plot between {numeric_cols[0]} and {numeric_cols[1]}\")\n",
    "    \n",
    "    # 6. Bar Chart for a categorical column\n",
    "    cat_col = categorical_cols[0] if categorical_cols else None\n",
    "    if cat_col:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        counts = df[cat_col].value_counts().sort_values(ascending=False)\n",
    "        sns.barplot(x=counts.index, y=counts.values)\n",
    "        plt.title(f'Bar Chart of {cat_col}')\n",
    "        plt.xlabel(cat_col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/6_barchart_{cat_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"6. Created bar chart for {cat_col}\")\n",
    "    \n",
    "    # 7. Pie Chart for a categorical column\n",
    "    if cat_col:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        df[cat_col].value_counts().plot.pie(autopct='%1.1f%%')\n",
    "        plt.title(f'Pie Chart of {cat_col}')\n",
    "        plt.ylabel('')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/7_piechart_{cat_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"7. Created pie chart for {cat_col}\")\n",
    "    \n",
    "    # 8. Violin Plot for a numeric column grouped by a categorical column\n",
    "    if numeric_col and cat_col and df[cat_col].nunique() <= 10:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.violinplot(x=cat_col, y=numeric_col, data=df)\n",
    "        plt.title(f'Violin Plot of {numeric_col} by {cat_col}')\n",
    "        plt.xlabel(cat_col)\n",
    "        plt.ylabel(numeric_col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/8_violinplot.png')\n",
    "        plt.close()\n",
    "        print(f\"8. Created violin plot for {numeric_col} by {cat_col}\")\n",
    "    \n",
    "    # 9. Quantile Plot (Empirical CDF)\n",
    "    if numeric_col:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sorted_data = np.sort(df[numeric_col].dropna())\n",
    "        y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "        plt.plot(sorted_data, y, marker='.', linestyle='none')\n",
    "        plt.title(f'Quantile Plot (Empirical CDF) of {numeric_col}')\n",
    "        plt.xlabel(numeric_col)\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/9_quantile_plot_{numeric_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"9. Created quantile plot for {numeric_col}\")\n",
    "    \n",
    "    # 10. Pair Plot for multiple numeric columns\n",
    "    if len(numeric_cols) >= 3:\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.pairplot(df[numeric_cols[:4]], height=2.5)  # Limit to first 4 columns\n",
    "        plt.suptitle('Pair Plot of Numeric Variables', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/10_pairplot.png')\n",
    "        plt.close()\n",
    "        print(\"10. Created pair plot for numeric variables\")\n",
    "\n",
    "# Calculate statistics for all numeric columns\n",
    "for col in numeric_cols:\n",
    "    calculate_statistics(df, col)\n",
    "\n",
    "# Generate the plots\n",
    "create_plots(df)\n",
    "\n",
    "print(\"\\nAnalysis complete. All plots have been saved to the 'plots' directory.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# تحلیل داده‌های افسردگی دانشجویان\n",
    "## گزارش تحلیل آماری و نمودارهای دیداری\n",
    "\n",
    "### مقدمه\n",
    "این گزارش، نتایج تحلیل مجموعه داده مربوط به افسردگی دانشجویان را ارائه می‌دهد. داده‌های مورد بررسی شامل ۲۷،۹۰۱ رکورد با ۱۸ متغیر است که عوامل مختلف مرتبط با سلامت روان و افسردگی دانشجویان را نشان می‌دهد.\n",
    "\n",
    "### کد آماده‌سازی محیط و بارگذاری داده‌ها\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# تنظیم استایل برای نمودارها\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# ایجاد پوشه برای ذخیره نمودارها\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# بارگذاری مجموعه داده\n",
    "df = pd.read_csv('student_depression_dataset.csv')\n",
    "\n",
    "# اطلاعات پایه و خلاصه آماری\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# بررسی مقادیر گمشده\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "```\n",
    "\n",
    "این کد کتابخانه‌های مورد نیاز را وارد می‌کند و پوشه‌ای برای ذخیره نمودارها ایجاد می‌کند. سپس مجموعه داده را بارگذاری کرده و اطلاعات اولیه مانند ابعاد، انواع داده‌ها، و مقادیر گمشده را نمایش می‌دهد.\n",
    "\n",
    "### شناسایی نوع متغیرها\n",
    "\n",
    "```python\n",
    "# شناسایی متغیرهای عددی و کیفی\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if 'id' in numeric_cols:\n",
    "    numeric_cols.remove('id')  # حذف شناسه طبق دستورالعمل\n",
    "    \n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric columns:\", numeric_cols)\n",
    "print(\"\\nCategorical columns:\", categorical_cols)\n",
    "\n",
    "# تابع تشخیص نوع متغیر\n",
    "def identify_variable_type(col):\n",
    "    if df[col].dtype == 'object':\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values == 2:\n",
    "            return \"Binary\"\n",
    "        elif unique_values <= 10:\n",
    "            return \"Nominal\" if not all(df[col].dropna().astype(str).str.isnumeric()) else \"Ordinal\"\n",
    "        else:\n",
    "            return \"Nominal\"\n",
    "    else:  # numeric\n",
    "        unique_values = df[col].nunique()\n",
    "        if unique_values == 2:\n",
    "            return \"Binary\"\n",
    "        elif unique_values <= 10:\n",
    "            return \"Ordinal\" if col not in ['id'] else \"ID\"\n",
    "        else:\n",
    "            return \"Numeric (Continuous)\"\n",
    "\n",
    "# شناسایی نوع متغیرها\n",
    "print(\"\\nVariable Types:\")\n",
    "for col in df.columns:\n",
    "    if col != 'id':  # حذف ستون شناسه\n",
    "        print(f\"{col}: {identify_variable_type(col)}\")\n",
    "```\n",
    "\n",
    "این بخش از کد به تفکیک متغیرها به دو گروه عددی و کیفی می‌پردازد. تابع `identify_variable_type` برای تشخیص نوع دقیق‌تر متغیرها (باینری، اسمی، ترتیبی یا پیوسته) استفاده می‌شود. برای هر ستون غیر از `id`، نوع آن تعیین و نمایش داده می‌شود.\n",
    "\n",
    "### ساختار داده‌ها\n",
    "مجموعه داده شامل انواع مختلفی از متغیرها است:\n",
    "\n",
    "#### متغیرهای دودویی (باینری)\n",
    "- آیا تا به حال افکار خودکشی داشته‌اید؟\n",
    "- سابقه خانوادگی بیماری روانی\n",
    "- افسردگی (متغیر هدف)\n",
    "\n",
    "#### متغیرهای عددی پیوسته\n",
    "- سن\n",
    "- فشار تحصیلی\n",
    "- فشار کاری\n",
    "- معدل\n",
    "- رضایت از تحصیل\n",
    "- رضایت شغلی\n",
    "- ساعات کار/مطالعه\n",
    "\n",
    "#### متغیرهای اسمی\n",
    "- جنسیت\n",
    "- شهر\n",
    "- حرفه\n",
    "- مدت زمان خواب\n",
    "- عادات غذایی\n",
    "- مدرک تحصیلی\n",
    "- استرس مالی\n",
    "\n",
    "#### متغیرهای ترتیبی\n",
    "- فشار تحصیلی (مقیاس ۰ تا ۵)\n",
    "- فشار کاری (مقیاس ۰ تا ۵)\n",
    "- رضایت از تحصیل (مقیاس ۰ تا ۵)\n",
    "- رضایت شغلی (مقیاس ۰ تا ۴)\n",
    "\n",
    "### محاسبه آمار توصیفی\n",
    "\n",
    "```python\n",
    "# محاسبه آمار توصیفی برای متغیرهای عددی\n",
    "def calculate_statistics(df, column):\n",
    "    if column in df.columns and column != 'id':\n",
    "        data = df[column].dropna()\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(data):\n",
    "            # محاسبه آماره‌ها\n",
    "            mean = data.mean()\n",
    "            median = data.median()\n",
    "            mode = data.mode()[0]\n",
    "            midrange = (data.max() + data.min()) / 2\n",
    "            q1 = data.quantile(0.25)\n",
    "            q3 = data.quantile(0.75)\n",
    "            min_val = data.min()\n",
    "            max_val = data.max()\n",
    "            \n",
    "            print(f\"\\nStatistics for {column}:\")\n",
    "            print(f\"Mean: {mean}\")\n",
    "            print(f\"Median: {median}\")\n",
    "            print(f\"Mode: {mode}\")\n",
    "            print(f\"Midrange: {midrange}\")\n",
    "            print(f\"Five Number Summary:\")\n",
    "            print(f\"  Minimum: {min_val}\")\n",
    "            print(f\"  Q1: {q1}\")\n",
    "            print(f\"  Median: {median}\")\n",
    "            print(f\"  Q3: {q3}\")\n",
    "            print(f\"  Maximum: {max_val}\")\n",
    "            \n",
    "            return {\n",
    "                'mean': mean,\n",
    "                'median': median,\n",
    "                'mode': mode,\n",
    "                'midrange': midrange,\n",
    "                'min': min_val,\n",
    "                'q1': q1,\n",
    "                'q3': q3,\n",
    "                'max': max_val\n",
    "            }\n",
    "        else:\n",
    "            print(f\"\\n{column} is not numeric, skipping statistics.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"\\n{column} not found in dataset or is ID column.\")\n",
    "        return None\n",
    "\n",
    "# محاسبه آمار توصیفی برای تمام متغیرهای عددی\n",
    "for col in numeric_cols:\n",
    "    calculate_statistics(df, col)\n",
    "```\n",
    "\n",
    "این تابع آمار توصیفی مهم برای متغیرهای عددی را محاسبه می‌کند، شامل:\n",
    "- میانگین (mean): مجموع مقادیر تقسیم بر تعداد آنها\n",
    "- میانه (median): مقدار وسط داده‌های مرتب شده\n",
    "- مد (mode): مقداری که بیشترین فراوانی را دارد\n",
    "- میان‌دامنه (midrange): میانگین کمترین و بیشترین مقدار\n",
    "- خلاصه پنج عددی: کمینه، چارک اول، میانه، چارک سوم، بیشینه\n",
    "\n",
    "این آماره‌ها برای درک توزیع داده‌ها و شناسایی الگوهای آنها ضروری است.\n",
    "\n",
    "### آمار توصیفی متغیرهای کلیدی\n",
    "\n",
    "#### سن\n",
    "- میانگین: ۲۵.۸۲\n",
    "- میانه: ۲۵.۰\n",
    "- مد: ۲۴.۰\n",
    "- میان‌دامنه: ۳۸.۵\n",
    "- خلاصه پنج عددی:\n",
    "  - کمینه: ۱۸.۰\n",
    "  - چارک اول: ۲۱.۰\n",
    "  - میانه: ۲۵.۰\n",
    "  - چارک سوم: ۳۰.۰\n",
    "  - بیشینه: ۵۹.۰\n",
    "\n",
    "#### فشار تحصیلی\n",
    "- میانگین: ۳.۱۴\n",
    "- میانه: ۳.۰\n",
    "- مد: ۳.۰\n",
    "- میان‌دامنه: ۲.۵\n",
    "- خلاصه پنج عددی:\n",
    "  - کمینه: ۰.۰\n",
    "  - چارک اول: ۲.۰\n",
    "  - میانه: ۳.۰\n",
    "  - چارک سوم: ۴.۰\n",
    "  - بیشینه: ۵.۰\n",
    "\n",
    "#### معدل\n",
    "- میانگین: ۷.۶۶\n",
    "- میانه: ۷.۷۷\n",
    "- مد: ۸.۰۴\n",
    "- میان‌دامنه: ۵.۰\n",
    "- خلاصه پنج عددی:\n",
    "  - کمینه: ۰.۰\n",
    "  - چارک اول: ۶.۲۹\n",
    "  - میانه: ۷.۷۷\n",
    "  - چارک سوم: ۸.۹۲\n",
    "  - بیشینه: ۱۰.۰\n",
    "\n",
    "#### ساعات کار/مطالعه\n",
    "- میانگین: ۷.۱۶\n",
    "- میانه: ۸.۰\n",
    "- مد: ۱۰.۰\n",
    "- میان‌دامنه: ۶.۰\n",
    "- خلاصه پنج عددی:\n",
    "  - کمینه: ۰.۰\n",
    "  - چارک اول: ۴.۰\n",
    "  - میانه: ۸.۰\n",
    "  - چارک سوم: ۱۰.۰\n",
    "  - بیشینه: ۱۲.۰\n",
    "\n",
    "#### افسردگی\n",
    "- میانگین: ۰.۵۹\n",
    "- میانه: ۱.۰\n",
    "- مد: ۱\n",
    "- میان‌دامنه: ۰.۵\n",
    "- خلاصه پنج عددی:\n",
    "  - کمینه: ۰\n",
    "  - چارک اول: ۰.۰\n",
    "  - میانه: ۱.۰\n",
    "  - چارک سوم: ۱.۰\n",
    "  - بیشینه: ۱\n",
    "\n",
    "### ایجاد ۱۰ نوع نمودار مختلف\n",
    "\n",
    "```python\n",
    "def create_plots(df):\n",
    "    # 1. هیستوگرام برای یک متغیر عددی\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    numeric_col = numeric_cols[0] if numeric_cols else None\n",
    "    if numeric_col:\n",
    "        sns.histplot(df[numeric_col], kde=True)\n",
    "        plt.title(f'Histogram of {numeric_col}')\n",
    "        plt.xlabel(numeric_col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/1_histogram_{numeric_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"1. Created histogram for {numeric_col}\")\n",
    "    \n",
    "    # 2. نمودار جعبه‌ای برای متغیرهای عددی\n",
    "    if len(numeric_cols) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(data=df[numeric_cols[:5]])  # محدود به ۵ ستون اول عددی\n",
    "        plt.title('Box Plot of Numeric Variables')\n",
    "        plt.ylabel('Value')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/2_boxplot.png')\n",
    "        plt.close()\n",
    "        print(\"2. Created box plot for numeric variables\")\n",
    "    \n",
    "    # 3. نمودار QQ-Plot برای یک متغیر عددی\n",
    "    if numeric_col:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        stats.probplot(df[numeric_col].dropna(), dist=\"norm\", plot=plt)\n",
    "        plt.title(f'Q-Q Plot of {numeric_col}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/3_qqplot_{numeric_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"3. Created QQ Plot for {numeric_col}\")\n",
    "    \n",
    "    # 4. نمودار همبستگی برای متغیرهای عددی\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr = df[numeric_cols].corr()\n",
    "        mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "        sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "        plt.title('Correlation Heatmap of Numeric Variables')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/4_correlation_heatmap.png')\n",
    "        plt.close()\n",
    "        print(\"4. Created correlation heatmap\")\n",
    "    \n",
    "    # 5. نمودار پراکندگی بین دو متغیر عددی\n",
    "    if len(numeric_cols) >= 2:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=df[numeric_cols[0]], y=df[numeric_cols[1]])\n",
    "        plt.title(f'Scatter Plot: {numeric_cols[0]} vs {numeric_cols[1]}')\n",
    "        plt.xlabel(numeric_cols[0])\n",
    "        plt.ylabel(numeric_cols[1])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/5_scatterplot.png')\n",
    "        plt.close()\n",
    "        print(f\"5. Created scatter plot between {numeric_cols[0]} and {numeric_cols[1]}\")\n",
    "    \n",
    "    # 6. نمودار میله‌ای برای یک متغیر کیفی\n",
    "    cat_col = categorical_cols[0] if categorical_cols else None\n",
    "    if cat_col:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        counts = df[cat_col].value_counts().sort_values(ascending=False)\n",
    "        sns.barplot(x=counts.index, y=counts.values)\n",
    "        plt.title(f'Bar Chart of {cat_col}')\n",
    "        plt.xlabel(cat_col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/6_barchart_{cat_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"6. Created bar chart for {cat_col}\")\n",
    "    \n",
    "    # 7. نمودار دایره‌ای برای یک متغیر کیفی\n",
    "    if cat_col:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        df[cat_col].value_counts().plot.pie(autopct='%1.1f%%')\n",
    "        plt.title(f'Pie Chart of {cat_col}')\n",
    "        plt.ylabel('')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/7_piechart_{cat_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"7. Created pie chart for {cat_col}\")\n",
    "    \n",
    "    # 8. نمودار ویولن برای یک متغیر عددی گروه‌بندی شده توسط یک متغیر کیفی\n",
    "    if numeric_col and cat_col and df[cat_col].nunique() <= 10:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.violinplot(x=cat_col, y=numeric_col, data=df)\n",
    "        plt.title(f'Violin Plot of {numeric_col} by {cat_col}')\n",
    "        plt.xlabel(cat_col)\n",
    "        plt.ylabel(numeric_col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/8_violinplot.png')\n",
    "        plt.close()\n",
    "        print(f\"8. Created violin plot for {numeric_col} by {cat_col}\")\n",
    "    \n",
    "    # 9. نمودار چندک (ECDF) برای یک متغیر عددی\n",
    "    if numeric_col:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sorted_data = np.sort(df[numeric_col].dropna())\n",
    "        y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "        plt.plot(sorted_data, y, marker='.', linestyle='none')\n",
    "        plt.title(f'Quantile Plot (Empirical CDF) of {numeric_col}')\n",
    "        plt.xlabel(numeric_col)\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/9_quantile_plot_{numeric_col}.png')\n",
    "        plt.close()\n",
    "        print(f\"9. Created quantile plot for {numeric_col}\")\n",
    "    \n",
    "    # 10. نمودار جفتی برای چندین متغیر عددی\n",
    "    if len(numeric_cols) >= 3:\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.pairplot(df[numeric_cols[:4]], height=2.5)  # محدود به ۴ ستون اول\n",
    "        plt.suptitle('Pair Plot of Numeric Variables', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/10_pairplot.png')\n",
    "        plt.close()\n",
    "        print(\"10. Created pair plot for numeric variables\")\n",
    "\n",
    "# ایجاد نمودارها\n",
    "create_plots(df)\n",
    "```\n",
    "\n",
    "این تابع بزرگ ۱۰ نوع مختلف نمودار را ایجاد می‌کند. هر نمودار هدف خاصی دارد:\n",
    "\n",
    "1. **هیستوگرام**: توزیع فراوانی یک متغیر عددی را نشان می‌دهد\n",
    "2. **نمودار جعبه‌ای**: پنج آماره اصلی (کمینه، Q1، میانه، Q3، بیشینه) و مقادیر پرت را نمایش می‌دهد\n",
    "3. **QQ-Plot**: مقایسه توزیع یک متغیر با توزیع نرمال\n",
    "4. **نمودار همبستگی**: نمایش همبستگی بین متغیرهای عددی مختلف\n",
    "5. **نمودار پراکندگی**: رابطه بین دو متغیر عددی را نشان می‌دهد\n",
    "6. **نمودار میله‌ای**: فراوانی مقادیر مختلف یک متغیر کیفی\n",
    "7. **نمودار دایره‌ای**: نسبت مقادیر یک متغیر کیفی را نشان می‌دهد\n",
    "8. **نمودار ویولن**: توزیع یک متغیر عددی را برای سطوح مختلف یک متغیر کیفی نشان می‌دهد\n",
    "9. **نمودار چندک**: تابع توزیع تجمعی تجربی را نمایش می‌دهد\n",
    "10. **نمودار جفتی**: روابط دو به دوی چند متغیر عددی را نشان می‌دهد\n",
    "\n",
    "هر یک از این نمودارها در پوشه `plots` ذخیره می‌شوند و می‌توانند برای تحلیل‌های بعدی مورد استفاده قرار گیرند.\n",
    "\n",
    "### تحلیل نمودارها\n",
    "\n",
    "#### ۱. هیستوگرام سن\n",
    "![هیستوگرام سن](./plots/1_histogram_Age.png)\n",
    "\n",
    "این نمودار توزیع فراوانی سن دانشجویان را نشان می‌دهد. بیشترین فراوانی مربوط به سنین ۲۴-۲۵ سال است. توزیع سن کمی چوله به راست است، یعنی تعداد کمتری از دانشجویان در سنین بالاتر قرار دارند.\n",
    "\n",
    "#### ۲. نمودار جعبه‌ای متغیرهای عددی\n",
    "![نمودار جعبه‌ای](./plots/2_boxplot.png)\n",
    "\n",
    "این نمودار توزیع متغیرهای عددی اصلی را نشان می‌دهد. می‌توان مشاهده کرد:\n",
    "- سن: توزیع نسبتاً متقارن با برخی مقادیر پرت در سنین بالاتر\n",
    "- فشار تحصیلی: بیشتر داده‌ها در محدوده ۲ تا ۴ قرار دارند\n",
    "- معدل: میانه حدود ۷.۷۷ و دارای توزیع نسبتاً متقارن\n",
    "- رضایت از تحصیل: اکثر داده‌ها بین ۲ تا ۴ قرار دارند\n",
    "- ساعات کار/مطالعه: اکثر دانشجویان بین ۴ تا ۱۰ ساعت مطالعه می‌کنند\n",
    "\n",
    "#### ۳. نمودار QQ-Plot برای سن\n",
    "![نمودار QQ-Plot](./plots/3_qqplot_Age.png)\n",
    "\n",
    "این نمودار نشان می‌دهد که توزیع سن تا حدودی از توزیع نرمال فاصله دارد، به خصوص در انتهای بالایی توزیع. این انحراف نشان‌دهنده چولگی مثبت در داده‌های سن است.\n",
    "\n",
    "#### ۴. نمودار همبستگی متغیرهای عددی\n",
    "![نمودار همبستگی](./plots/4_correlation_heatmap.png)\n",
    "\n",
    "این نمودار درجه همبستگی بین متغیرهای عددی را نشان می‌دهد:\n",
    "- همبستگی مثبت نسبتاً قوی بین رضایت از تحصیل و فشار تحصیلی (۰.۳۳)\n",
    "- همبستگی منفی ضعیف بین سن و معدل (-۰.۰۶)\n",
    "- همبستگی مثبت ضعیف بین ساعات کار/مطالعه و فشار تحصیلی (۰.۰۵)\n",
    "- همبستگی نسبتاً کم بین متغیر افسردگی و سایر متغیرهای عددی\n",
    "\n",
    "#### ۵. نمودار پراکندگی سن و فشار تحصیلی\n",
    "![نمودار پراکندگی](./plots/5_scatterplot.png)\n",
    "\n",
    "این نمودار رابطه بین سن و فشار تحصیلی را نشان می‌دهد. الگوی خاصی مشاهده نمی‌شود که نشان‌دهنده عدم وجود رابطه قوی خطی بین این دو متغیر است.\n",
    "\n",
    "#### ۶. نمودار میله‌ای جنسیت\n",
    "![نمودار میله‌ای](./plots/6_barchart_Gender.png)\n",
    "\n",
    "این نمودار توزیع فراوانی جنسیت دانشجویان را نشان می‌دهد. به نظر می‌رسد توزیع جنسیتی نسبتاً متعادل است.\n",
    "\n",
    "#### ۷. نمودار دایره‌ای جنسیت\n",
    "![نمودار دایره‌ای](./plots/7_piechart_Gender.png)\n",
    "\n",
    "این نمودار درصد دانشجویان بر اساس جنسیت را نشان می‌دهد و تأییدی بر توزیع نسبتاً متعادل جنسیتی در نمونه است.\n",
    "\n",
    "#### ۸. نمودار ویولن سن بر اساس جنسیت\n",
    "![نمودار ویولن](./plots/8_violinplot.png)\n",
    "\n",
    "این نمودار توزیع سن را به تفکیک جنسیت نشان می‌دهد. می‌توان مشاهده کرد که میانه سنی در دو گروه تفاوت چندانی ندارد، اما توزیع سنی در مردان اندکی گسترده‌تر است.\n",
    "\n",
    "#### ۹. نمودار چندک (تجمعی) برای سن\n",
    "![نمودار چندک](./plots/9_quantile_plot_Age.png)\n",
    "\n",
    "این نمودار توزیع تجمعی سن را نشان می‌دهد. شیب تندتر در محدوده سنی ۲۰ تا ۳۰ سال نشان‌دهنده تمرکز بیشتر دانشجویان در این بازه سنی است.\n",
    "\n",
    "#### ۱۰. نمودار جفتی متغیرهای عددی\n",
    "![نمودار جفتی](./plots/10_pairplot.png)\n",
    "\n",
    "این مجموعه نمودارها روابط دو به دوی متغیرهای عددی را نشان می‌دهد:\n",
    "- نمودارهای قطری هیستوگرام هر متغیر را نشان می‌دهند\n",
    "- نمودارهای غیر قطری، نمودارهای پراکندگی بین هر جفت متغیر هستند\n",
    "- الگوهای همبستگی قوی بین متغیرها مشاهده نمی‌شود\n",
    "\n",
    "### نتیجه‌گیری\n",
    "بر اساس تحلیل‌های انجام شده، می‌توان موارد زیر را نتیجه‌گیری کرد:\n",
    "\n",
    "۱. **ویژگی‌های جمعیت‌شناختی**: اکثر دانشجویان در محدوده سنی ۲۱ تا ۳۰ سال قرار دارند، با میانگین سنی ۲۵.۸ سال. توزیع جنسیتی نسبتاً متعادل است.\n",
    "\n",
    "۲. **فشار تحصیلی**: میانگین فشار تحصیلی ۳.۱۴ از ۵ است که نشان‌دهنده سطح متوسط رو به بالای فشار تحصیلی در بین دانشجویان است.\n",
    "\n",
    "۳. **معدل**: میانگین معدل دانشجویان ۷.۶۶ از ۱۰ است که نشان‌دهنده عملکرد تحصیلی نسبتاً خوب است.\n",
    "\n",
    "۴. **ساعات مطالعه**: دانشجویان به طور میانگین ۷.۱۶ ساعت در روز به کار یا مطالعه می‌پردازند، با بیشترین فراوانی در ۱۰ ساعت.\n",
    "\n",
    "۵. **افسردگی**: حدود ۵۹٪ از دانشجویان (میانگین ۰.۵۹) علائم افسردگی را گزارش کرده‌اند که نشان‌دهنده شیوع نسبتاً بالای مشکلات سلامت روان در این جامعه آماری است.\n",
    "\n",
    "۶. **همبستگی‌ها**: ارتباط قوی و معناداری بین متغیرهای عددی و افسردگی مشاهده نشد، که نشان می‌دهد عوامل مؤثر بر افسردگی دانشجویان پیچیده و چندبعدی هستند.\n",
    "\n",
    "این یافته‌ها می‌تواند برای طراحی برنامه‌های پیشگیری و مداخله در زمینه سلامت روان دانشجویان مورد استفاده قرار گیرد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Original dataset shape: (27901, 18)\n",
      "\n",
      "===== DATA CLEANING =====\n",
      "Number of duplicate records: 0\n",
      "\n",
      "Checking for outliers and inconsistent values in numeric columns...\n",
      "Age: Range [18.0, 59.0]\n",
      "  Found 12 potential outliers in Age\n",
      "  Outliers in Age capped to [7.50, 43.50]\n",
      "Academic Pressure: Range [0.0, 5.0]\n",
      "Work Pressure: Range [0.0, 5.0]\n",
      "  Found 3 potential outliers in Work Pressure\n",
      "  Outliers in Work Pressure capped to [0.00, 0.00]\n",
      "CGPA: Range [0.0, 10.0]\n",
      "  Found 9 potential outliers in CGPA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n",
      "C:\\Users\\asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n",
      "  positions = grouped.grouper.result_index.to_numpy(dtype=float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Outliers in CGPA capped to [2.35, 12.87]\n",
      "Study Satisfaction: Range [0.0, 5.0]\n",
      "Job Satisfaction: Range [0.0, 4.0]\n",
      "  Found 8 potential outliers in Job Satisfaction\n",
      "  Outliers in Job Satisfaction capped to [0.00, 0.00]\n",
      "Work/Study Hours: Range [0.0, 12.0]\n",
      "\n",
      "Checking for logical inconsistencies...\n",
      "\n",
      "Checking for zero values that might represent missing data...\n",
      "Work Pressure: 27901 zeros (100.00%)\n",
      "Job Satisfaction: 27901 zeros (100.00%)\n",
      "Work/Study Hours: 1700 zeros (6.09%)\n",
      "\n",
      "===== DATA TRANSFORMATION =====\n",
      "\n",
      "Standardizing numeric features...\n",
      "Standardized 7 numeric columns\n",
      "\n",
      "Encoding categorical variables...\n",
      "Processing Gender...\n",
      "  2 unique values\n",
      "  Applied label encoding to Gender\n",
      "  Mapping: {'Female': np.int64(0), 'Male': np.int64(1)}\n",
      "Processing City...\n",
      "  52 unique values\n",
      "  Applied frequency encoding to City\n",
      "Processing Profession...\n",
      "  14 unique values\n",
      "  Applied frequency encoding to Profession\n",
      "Processing Sleep Duration...\n",
      "  5 unique values\n",
      "  Applied one-hot encoding to Sleep Duration, created 4 new features\n",
      "Processing Dietary Habits...\n",
      "  4 unique values\n",
      "  Applied one-hot encoding to Dietary Habits, created 3 new features\n",
      "Processing Degree...\n",
      "  28 unique values\n",
      "  Applied frequency encoding to Degree\n",
      "Processing Have you ever had suicidal thoughts ?...\n",
      "  2 unique values\n",
      "  Applied label encoding to Have you ever had suicidal thoughts ?\n",
      "  Mapping: {'No': np.int64(0), 'Yes': np.int64(1)}\n",
      "Processing Financial Stress...\n",
      "  6 unique values\n",
      "  Applied one-hot encoding to Financial Stress, created 5 new features\n",
      "Processing Family History of Mental Illness...\n",
      "  2 unique values\n",
      "  Applied label encoding to Family History of Mental Illness\n",
      "  Mapping: {'No': np.int64(0), 'Yes': np.int64(1)}\n",
      "\n",
      "Creating engineered features...\n",
      "Created Combined_Stress_Index\n",
      "Created Well_being_Index\n",
      "\n",
      "Processed data saved to processed_data/student_depression_processed.csv\n",
      "Final dataset shape: (27901, 29)\n",
      "\n",
      "Summary of Transformations Applied:\n",
      "1. Removed duplicates (if any)\n",
      "2. Handled outliers by capping\n",
      "3. Fixed logical inconsistencies\n",
      "4. Standardized numeric features\n",
      "5. Encoded categorical variables\n",
      "6. Created engineered features\n",
      "\n",
      "First few rows of processed dataset:\n",
      "   id  Gender       Age  Academic Pressure  Work Pressure      CGPA  \\\n",
      "0   2       1  1.468036           1.345543            0.0  0.894910   \n",
      "1   8       0 -0.371929          -0.826104            0.0 -1.197308   \n",
      "2  26       1  1.059155          -0.102222            0.0 -0.427208   \n",
      "\n",
      "   Study Satisfaction  Job Satisfaction  \\\n",
      "0           -0.693425               0.0   \n",
      "1            1.510636               0.0   \n",
      "2            1.510636               0.0   \n",
      "\n",
      "   Have you ever had suicidal thoughts ?  Work/Study Hours  ...  \\\n",
      "0                                      1         -1.121213  ...   \n",
      "1                                      0         -1.121213  ...   \n",
      "2                                      0          0.497095  ...   \n",
      "\n",
      "   Dietary Habits_Others  Dietary Habits_Unhealthy  Degree_freq  \\\n",
      "0                  False                     False     0.029031   \n",
      "1                  False                     False     0.031827   \n",
      "2                  False                     False     0.021505   \n",
      "\n",
      "   Financial Stress_2.0  Financial Stress_3.0  Financial Stress_4.0  \\\n",
      "0                 False                 False                 False   \n",
      "1                  True                 False                 False   \n",
      "2                 False                 False                 False   \n",
      "\n",
      "   Financial Stress_5.0  Financial Stress_?  Combined_Stress_Index  \\\n",
      "0                 False               False               0.672771   \n",
      "1                 False               False              -0.413052   \n",
      "2                 False               False              -0.051111   \n",
      "\n",
      "   Well_being_Index  \n",
      "0         -0.346713  \n",
      "1          0.755318  \n",
      "2          0.755318  \n",
      "\n",
      "[3 rows x 29 columns]\n",
      "\n",
      "Column list of processed dataset:\n",
      "['id', 'Gender', 'Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Have you ever had suicidal thoughts ?', 'Work/Study Hours', 'Family History of Mental Illness', 'Depression', 'City_freq', 'Profession_freq', \"Sleep Duration_'7-8 hours'\", \"Sleep Duration_'Less than 5 hours'\", \"Sleep Duration_'More than 8 hours'\", 'Sleep Duration_Others', 'Dietary Habits_Moderate', 'Dietary Habits_Others', 'Dietary Habits_Unhealthy', 'Degree_freq', 'Financial Stress_2.0', 'Financial Stress_3.0', 'Financial Stress_4.0', 'Financial Stress_5.0', 'Financial Stress_?', 'Combined_Stress_Index', 'Well_being_Index']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import os\n",
    "\n",
    "# Create directory for processed data and visualizations\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "os.makedirs('cleaning_plots', exist_ok=True)\n",
    "\n",
    "print(\"Loading the dataset...\")\n",
    "# Load the dataset and create a copy for preprocessing\n",
    "df = pd.read_csv('student_depression_dataset.csv')\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# ============= DATA CLEANING =============\n",
    "print(\"\\n===== DATA CLEANING =====\")\n",
    "\n",
    "# 1. Check for duplicate records\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "print(f\"Number of duplicate records: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {df_processed.shape}\")\n",
    "\n",
    "# 2. Check for inconsistent values in numeric columns\n",
    "numeric_cols = ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', \n",
    "                'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours']\n",
    "\n",
    "print(\"\\nChecking for outliers and inconsistent values in numeric columns...\")\n",
    "for col in numeric_cols:\n",
    "    # Print range and check for unusual values\n",
    "    min_val = df_processed[col].min()\n",
    "    max_val = df_processed[col].max()\n",
    "    print(f\"{col}: Range [{min_val}, {max_val}]\")\n",
    "    \n",
    "    # Identify potential outliers using IQR method\n",
    "    Q1 = df_processed[col].quantile(0.25)\n",
    "    Q3 = df_processed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_processed[(df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)]\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        print(f\"  Found {len(outliers)} potential outliers in {col}\")\n",
    "        \n",
    "        # Visualize outliers\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=df_processed[col])\n",
    "        plt.title(f'Boxplot of {col} - Outlier Detection')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'cleaning_plots/outliers_{col.replace(\"/\", \"_\")}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Cap outliers to the boundary values (alternative to dropping)\n",
    "        df_processed[col] = df_processed[col].clip(lower_bound, upper_bound)\n",
    "        print(f\"  Outliers in {col} capped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "# 3. Check for logical inconsistencies\n",
    "print(\"\\nChecking for logical inconsistencies...\")\n",
    "\n",
    "# Work/Study Hours should be between 0 and 24\n",
    "invalid_hours = df_processed[df_processed['Work/Study Hours'] > 24]\n",
    "if not invalid_hours.empty:\n",
    "    print(f\"Found {len(invalid_hours)} records with Work/Study Hours > 24\")\n",
    "    df_processed.loc[df_processed['Work/Study Hours'] > 24, 'Work/Study Hours'] = 24\n",
    "    print(\"  Capped Work/Study Hours to 24\")\n",
    "\n",
    "# Age should be reasonable for students (e.g., 16-80)\n",
    "invalid_age = df_processed[(df_processed['Age'] < 16) | (df_processed['Age'] > 80)]\n",
    "if not invalid_age.empty:\n",
    "    print(f\"Found {len(invalid_age)} records with unusual Age values\")\n",
    "    df_processed['Age'] = df_processed['Age'].clip(16, 80)\n",
    "    print(\"  Capped Age to [16, 80]\")\n",
    "\n",
    "# CGPA should be between 0 and 10 (assuming 10-point scale based on data)\n",
    "if df_processed['CGPA'].max() > 10:\n",
    "    print(f\"Found CGPA values > 10, maximum value: {df_processed['CGPA'].max()}\")\n",
    "    df_processed['CGPA'] = df_processed['CGPA'].clip(0, 10)\n",
    "    print(\"  Capped CGPA to [0, 10]\")\n",
    "\n",
    "# 4. Check for zero values that might be missing values\n",
    "print(\"\\nChecking for zero values that might represent missing data...\")\n",
    "for col in numeric_cols:\n",
    "    zero_count = (df_processed[col] == 0).sum()\n",
    "    zero_percentage = (zero_count / len(df_processed)) * 100\n",
    "    if zero_percentage > 1:  # If more than 1% are zeros\n",
    "        print(f\"{col}: {zero_count} zeros ({zero_percentage:.2f}%)\")\n",
    "\n",
    "# ============= DATA TRANSFORMATION =============\n",
    "print(\"\\n===== DATA TRANSFORMATION =====\")\n",
    "\n",
    "# 1. Standardize numeric features\n",
    "print(\"\\nStandardizing numeric features...\")\n",
    "scaler = StandardScaler()\n",
    "df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])\n",
    "print(f\"Standardized {len(numeric_cols)} numeric columns\")\n",
    "\n",
    "# 2. Encoding categorical variables\n",
    "categorical_cols = ['Gender', 'City', 'Profession', 'Sleep Duration', \n",
    "                   'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?',\n",
    "                   'Financial Stress', 'Family History of Mental Illness']\n",
    "\n",
    "print(\"\\nEncoding categorical variables...\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"Processing {col}...\")\n",
    "    # Check number of unique values\n",
    "    n_unique = df_processed[col].nunique()\n",
    "    print(f\"  {n_unique} unique values\")\n",
    "    \n",
    "    # For binary variables, use simple label encoding\n",
    "    if n_unique == 2:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col])\n",
    "        print(f\"  Applied label encoding to {col}\")\n",
    "        mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        print(f\"  Mapping: {mapping}\")\n",
    "    \n",
    "    # For variables with limited categories, use one-hot encoding\n",
    "    elif n_unique <= 10:\n",
    "        # Get dummies and drop the first to avoid multicollinearity\n",
    "        dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
    "        df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "        df_processed.drop(col, axis=1, inplace=True)\n",
    "        print(f\"  Applied one-hot encoding to {col}, created {len(dummies.columns)} new features\")\n",
    "    \n",
    "    # For high-cardinality variables, consider frequency encoding\n",
    "    else:\n",
    "        # Create a frequency map\n",
    "        freq_map = df_processed[col].value_counts(normalize=True).to_dict()\n",
    "        df_processed[f'{col}_freq'] = df_processed[col].map(freq_map)\n",
    "        df_processed.drop(col, axis=1, inplace=True)\n",
    "        print(f\"  Applied frequency encoding to {col}\")\n",
    "\n",
    "# 3. Feature Engineering\n",
    "print(\"\\nCreating engineered features...\")\n",
    "\n",
    "# Combined Stress Index (combining academic and work pressure)\n",
    "if 'Academic Pressure' in df_processed.columns and 'Work Pressure' in df_processed.columns:\n",
    "    df_processed['Combined_Stress_Index'] = (df_processed['Academic Pressure'] + df_processed['Work Pressure']) / 2\n",
    "    print(\"Created Combined_Stress_Index\")\n",
    "\n",
    "# Well-being Index (combining satisfaction measures)\n",
    "if 'Study Satisfaction' in df_processed.columns and 'Job Satisfaction' in df_processed.columns:\n",
    "    df_processed['Well_being_Index'] = (df_processed['Study Satisfaction'] + df_processed['Job Satisfaction']) / 2\n",
    "    print(\"Created Well_being_Index\")\n",
    "\n",
    "# 4. Save the processed data\n",
    "processed_file = 'processed_data/student_depression_processed.csv'\n",
    "df_processed.to_csv(processed_file, index=False)\n",
    "print(f\"\\nProcessed data saved to {processed_file}\")\n",
    "print(f\"Final dataset shape: {df_processed.shape}\")\n",
    "\n",
    "# Summary of transformations\n",
    "print(\"\\nSummary of Transformations Applied:\")\n",
    "print(\"1. Removed duplicates (if any)\")\n",
    "print(\"2. Handled outliers by capping\")\n",
    "print(\"3. Fixed logical inconsistencies\")\n",
    "print(\"4. Standardized numeric features\")\n",
    "print(\"5. Encoded categorical variables\")\n",
    "print(\"6. Created engineered features\")\n",
    "\n",
    "# Print head of processed dataset\n",
    "print(\"\\nFirst few rows of processed dataset:\")\n",
    "print(df_processed.head(3))\n",
    "print(\"\\nColumn list of processed dataset:\")\n",
    "print(df_processed.columns.tolist()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# گزارش پیش‌پردازش داده‌های افسردگی دانشجویان\n",
    "## پاکسازی و تبدیل داده‌ها\n",
    "\n",
    "### مقدمه\n",
    "این گزارش شرح مراحل پیش‌پردازش داده‌ها (شامل پاکسازی و تبدیل) برای مجموعه داده افسردگی دانشجویان را ارائه می‌دهد. این مراحل برای آماده‌سازی داده‌ها جهت استفاده در الگوریتم‌های یادگیری ماشین مانند خوشه‌بندی (DBSCAN، K-means) و طبقه‌بندی (ID3، بیزین) ضروری است.\n",
    "\n",
    "مجموعه داده اصلی شامل ۲۷،۹۰۱ رکورد با ۱۸ متغیر است که پس از پیش‌پردازش به ۲۹ متغیر افزایش یافته است.\n",
    "\n",
    "### کد آماده‌سازی محیط و بارگذاری داده‌ها\n",
    "\n",
    "در ابتدا، کتابخانه‌های مورد نیاز را وارد می‌کنیم و پوشه‌هایی برای ذخیره نتایج ایجاد می‌کنیم:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import os\n",
    "\n",
    "# ایجاد پوشه‌ها برای ذخیره داده‌های پیش‌پردازش شده و تصاویر\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "os.makedirs('cleaning_plots', exist_ok=True)\n",
    "\n",
    "# بارگذاری مجموعه داده و ایجاد یک کپی برای پیش‌پردازش\n",
    "df = pd.read_csv('student_depression_dataset.csv')\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "```\n",
    "\n",
    "این کد کتابخانه‌های ضروری برای تحلیل داده و پیش‌پردازش را وارد می‌کند. از `pandas` و `numpy` برای کار با داده‌ها، از `matplotlib` و `seaborn` برای تجسم، و از `sklearn` برای الگوریتم‌های پیش‌پردازش استفاده می‌کنیم. ایجاد پوشه‌ها با استفاده از `os.makedirs` برای سازماندهی فایل‌های خروجی انجام می‌شود.\n",
    "\n",
    "### مرحله ۱: پاکسازی داده‌ها\n",
    "\n",
    "#### بررسی و حذف رکوردهای تکراری\n",
    "\n",
    "```python\n",
    "# بررسی رکوردهای تکراری\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "print(f\"Number of duplicate records: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {df_processed.shape}\")\n",
    "```\n",
    "\n",
    "این کد از روش `.duplicated()` پانداس برای شناسایی ردیف‌های تکراری استفاده می‌کند. اگر رکوردهای تکراری پیدا شود، با استفاده از روش `.drop_duplicates()` آنها را حذف می‌کند.\n",
    "\n",
    "در بررسی اولیه، هیچ رکورد تکراری در مجموعه داده یافت نشد.\n",
    "\n",
    "#### تشخیص و اصلاح مقادیر پرت (اوتلایر)\n",
    "\n",
    "```python\n",
    "# شناسایی متغیرهای عددی برای بررسی مقادیر پرت\n",
    "numeric_cols = ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', \n",
    "                'Study Satisfaction', 'Job Satisfaction', 'Work/Study Hours']\n",
    "\n",
    "print(\"\\nChecking for outliers and inconsistent values in numeric columns...\")\n",
    "for col in numeric_cols:\n",
    "    # چاپ دامنه و بررسی مقادیر غیرعادی\n",
    "    min_val = df_processed[col].min()\n",
    "    max_val = df_processed[col].max()\n",
    "    print(f\"{col}: Range [{min_val}, {max_val}]\")\n",
    "    \n",
    "    # شناسایی مقادیر پرت احتمالی با استفاده از روش IQR\n",
    "    Q1 = df_processed[col].quantile(0.25)\n",
    "    Q3 = df_processed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_processed[(df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)]\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        print(f\"  Found {len(outliers)} potential outliers in {col}\")\n",
    "        \n",
    "        # تجسم مقادیر پرت\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=df_processed[col])\n",
    "        plt.title(f'Boxplot of {col} - Outlier Detection')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'cleaning_plots/outliers_{col.replace(\"/\", \"_\")}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # محدود کردن مقادیر پرت به مقادیر مرزی (جایگزین حذف)\n",
    "        df_processed[col] = df_processed[col].clip(lower_bound, upper_bound)\n",
    "        print(f\"  Outliers in {col} capped to [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "```\n",
    "\n",
    "این کد از روش محدوده بین چارکی (IQR) برای شناسایی مقادیر پرت استفاده می‌کند. برای هر متغیر عددی، چارک اول (Q1) و چارک سوم (Q3) محاسبه می‌شود. سپس دامنه حدی پایین (Q1 - 1.5*IQR) و بالا (Q3 + 1.5*IQR) تعیین می‌شود. مقادیری که خارج از این محدوده قرار دارند به عنوان مقادیر پرت شناسایی می‌شوند.\n",
    "\n",
    "از تابع `clip()` پانداس برای محدود کردن مقادیر پرت به حدود تعیین شده استفاده می‌کنیم. این روش به جای حذف کامل داده‌های پرت، آنها را به مقادیر مرزی تبدیل می‌کند که برای حفظ حجم داده‌ها مفید است.\n",
    "\n",
    "با استفاده از روش دامنه میان چارکی (IQR)، مقادیر پرت در متغیرهای عددی شناسایی و اصلاح شدند:\n",
    "\n",
    "1. **سن (Age)**:\n",
    "   - ۱۲ مقدار پرت شناسایی شد\n",
    "   - مقادیر به محدوده [۷.۵۰، ۴۳.۵۰] محدود شدند\n",
    "   - نمودار جعبه‌ای برای تشخیص مقادیر پرت:\n",
    "   \n",
    "   ![نمودار جعبه‌ای سن](./cleaning_plots/outliers_Age.png)\n",
    "\n",
    "2. **فشار کاری (Work Pressure)**:\n",
    "   - ۳ مقدار پرت شناسایی شد\n",
    "   - مقادیر به محدوده [۰.۰۰، ۰.۰۰] محدود شدند\n",
    "   - نمودار جعبه‌ای برای تشخیص مقادیر پرت:\n",
    "   \n",
    "   ![نمودار جعبه‌ای فشار کاری](./cleaning_plots/outliers_Work_Pressure.png)\n",
    "\n",
    "3. **معدل (CGPA)**:\n",
    "   - ۹ مقدار پرت شناسایی شد\n",
    "   - مقادیر به محدوده [۲.۳۵، ۱۲.۸۷] محدود شدند\n",
    "   - نمودار جعبه‌ای برای تشخیص مقادیر پرت:\n",
    "   \n",
    "   ![نمودار جعبه‌ای معدل](./cleaning_plots/outliers_CGPA.png)\n",
    "\n",
    "4. **رضایت شغلی (Job Satisfaction)**:\n",
    "   - ۸ مقدار پرت شناسایی شد\n",
    "   - مقادیر به محدوده [۰.۰۰، ۰.۰۰] محدود شدند\n",
    "   - نمودار جعبه‌ای برای تشخیص مقادیر پرت:\n",
    "   \n",
    "   ![نمودار جعبه‌ای رضایت شغلی](./cleaning_plots/outliers_Job_Satisfaction.png)\n",
    "\n",
    "#### بررسی ناسازگاری‌های منطقی\n",
    "\n",
    "```python\n",
    "# بررسی ناسازگاری‌های منطقی\n",
    "print(\"\\nChecking for logical inconsistencies...\")\n",
    "\n",
    "# ساعات کار/مطالعه باید بین 0 و 24 ساعت باشد\n",
    "invalid_hours = df_processed[df_processed['Work/Study Hours'] > 24]\n",
    "if not invalid_hours.empty:\n",
    "    print(f\"Found {len(invalid_hours)} records with Work/Study Hours > 24\")\n",
    "    df_processed.loc[df_processed['Work/Study Hours'] > 24, 'Work/Study Hours'] = 24\n",
    "    print(\"  Capped Work/Study Hours to 24\")\n",
    "\n",
    "# سن باید در محدوده منطقی برای دانشجویان باشد (مثلاً 16 تا 80)\n",
    "invalid_age = df_processed[(df_processed['Age'] < 16) | (df_processed['Age'] > 80)]\n",
    "if not invalid_age.empty:\n",
    "    print(f\"Found {len(invalid_age)} records with unusual Age values\")\n",
    "    df_processed['Age'] = df_processed['Age'].clip(16, 80)\n",
    "    print(\"  Capped Age to [16, 80]\")\n",
    "\n",
    "# معدل باید بین 0 و 10 باشد (با فرض مقیاس 10 نمره‌ای بر اساس داده‌ها)\n",
    "if df_processed['CGPA'].max() > 10:\n",
    "    print(f\"Found CGPA values > 10, maximum value: {df_processed['CGPA'].max()}\")\n",
    "    df_processed['CGPA'] = df_processed['CGPA'].clip(0, 10)\n",
    "    print(\"  Capped CGPA to [0, 10]\")\n",
    "```\n",
    "\n",
    "این کد محدودیت‌های منطقی مهمی را برای متغیرهای کلیدی بررسی می‌کند:\n",
    "- ساعات کار/مطالعه نباید بیشتر از 24 ساعت در روز باشد\n",
    "- سن دانشجویان باید در محدوده منطقی (16 تا 80 سال) باشد\n",
    "- معدل (CGPA) باید بین 0 تا 10 (با فرض سیستم نمره‌دهی 10 نمره‌ای) باشد\n",
    "\n",
    "اگر مقادیری خارج از این محدوده‌ها یافت شود، از توابع `.clip()` یا `.loc[]` برای محدود کردن مقادیر به محدوده‌های منطقی استفاده می‌شود.\n",
    "\n",
    "بررسی‌هایی برای تشخیص مقادیر غیرمنطقی در داده‌ها انجام شد:\n",
    "\n",
    "1. **ساعات کار/مطالعه**: بررسی شد که مقادیر بیش از ۲۴ ساعت در روز نباشد (موردی یافت نشد)\n",
    "2. **سن**: بررسی شد که سن بین محدوده منطقی ۱۶ تا ۸۰ سال باشد (موردی یافت نشد)\n",
    "3. **معدل**: بررسی شد که معدل بین ۰ تا ۱۰ باشد (موردی یافت نشد)\n",
    "\n",
    "#### بررسی مقادیر صفر که می‌توانند نشان‌دهنده داده‌های گمشده باشند\n",
    "\n",
    "```python\n",
    "# بررسی مقادیر صفر که ممکن است نشان‌دهنده داده‌های گمشده باشند\n",
    "print(\"\\nChecking for zero values that might represent missing data...\")\n",
    "for col in numeric_cols:\n",
    "    zero_count = (df_processed[col] == 0).sum()\n",
    "    zero_percentage = (zero_count / len(df_processed)) * 100\n",
    "    if zero_percentage > 1:  # اگر بیش از 1% مقادیر صفر باشند\n",
    "        print(f\"{col}: {zero_count} zeros ({zero_percentage:.2f}%)\")\n",
    "```\n",
    "\n",
    "این کد برای شناسایی الگوهای غیرعادی مقادیر صفر در متغیرهای عددی استفاده می‌شود. مقادیر صفر ممکن است نشان‌دهنده داده‌های گمشده باشند که به جای NaN به عنوان صفر ثبت شده‌اند. برای هر متغیر عددی، کد تعداد و درصد مقادیر صفر را محاسبه می‌کند و آنهایی که بیش از 1% داده‌ها را تشکیل می‌دهند گزارش می‌شوند.\n",
    "\n",
    "الگوهای قابل توجهی از مقادیر صفر شناسایی شدند:\n",
    "\n",
    "1. **فشار کاری (Work Pressure)**: ۱۰۰٪ مقادیر صفر هستند\n",
    "2. **رضایت شغلی (Job Satisfaction)**: ۱۰۰٪ مقادیر صفر هستند\n",
    "3. **ساعات کار/مطالعه (Work/Study Hours)**: ۶.۰۹٪ مقادیر صفر هستند\n",
    "\n",
    "مقادیر صفر در فشار کاری و رضایت شغلی نشان می‌دهد که این مجموعه داده احتمالاً فقط شامل دانشجویان بدون شغل است.\n",
    "\n",
    "### مرحله ۲: تبدیل داده‌ها\n",
    "\n",
    "#### استاندارد‌سازی متغیرهای عددی\n",
    "\n",
    "```python\n",
    "# استاندارد‌سازی متغیرهای عددی\n",
    "print(\"\\nStandardizing numeric features...\")\n",
    "scaler = StandardScaler()\n",
    "df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])\n",
    "print(f\"Standardized {len(numeric_cols)} numeric columns\")\n",
    "```\n",
    "\n",
    "این کد از `StandardScaler` کتابخانه scikit-learn برای استاندارد کردن متغیرهای عددی استفاده می‌کند. استاندارد‌سازی فرآیندی است که میانگین هر متغیر را به صفر و انحراف معیار آن را به یک تبدیل می‌کند:\n",
    "- `X_std = (X - μ) / σ`\n",
    "که در آن μ میانگین و σ انحراف معیار است.\n",
    "\n",
    "استاندارد‌سازی متغیرها به ویژه برای الگوریتم‌های یادگیری ماشین مانند K-means و DBSCAN که به مقیاس متغیرها حساس هستند، بسیار مهم است. این کار اطمینان می‌دهد که متغیرهای با مقیاس‌های بزرگتر تأثیر نامتناسبی بر نتایج نداشته باشند.\n",
    "\n",
    "تمامی ۷ متغیر عددی با استفاده از `StandardScaler` استاندارد شدند تا میانگین صفر و انحراف معیار یک داشته باشند:\n",
    "\n",
    "- سن (Age)\n",
    "- فشار تحصیلی (Academic Pressure)\n",
    "- فشار کاری (Work Pressure)\n",
    "- معدل (CGPA)\n",
    "- رضایت از تحصیل (Study Satisfaction)\n",
    "- رضایت شغلی (Job Satisfaction)\n",
    "- ساعات کار/مطالعه (Work/Study Hours)\n",
    "\n",
    "این استاندارد‌سازی برای بهبود عملکرد الگوریتم‌های مبتنی بر فاصله مانند K-means و DBSCAN ضروری است.\n",
    "\n",
    "#### کدگذاری متغیرهای کیفی\n",
    "\n",
    "```python\n",
    "# شناسایی متغیرهای کیفی\n",
    "categorical_cols = ['Gender', 'City', 'Profession', 'Sleep Duration', \n",
    "                   'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?',\n",
    "                   'Financial Stress', 'Family History of Mental Illness']\n",
    "\n",
    "print(\"\\nEncoding categorical variables...\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"Processing {col}...\")\n",
    "    # بررسی تعداد مقادیر منحصر به فرد\n",
    "    n_unique = df_processed[col].nunique()\n",
    "    print(f\"  {n_unique} unique values\")\n",
    "    \n",
    "    # برای متغیرهای دودویی، از کدگذاری برچسب ساده استفاده می‌کنیم\n",
    "    if n_unique == 2:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col])\n",
    "        print(f\"  Applied label encoding to {col}\")\n",
    "        mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        print(f\"  Mapping: {mapping}\")\n",
    "    \n",
    "    # برای متغیرهای با تعداد محدود طبقات، از کدگذاری یک‌در‌یک استفاده می‌کنیم\n",
    "    elif n_unique <= 10:\n",
    "        # ایجاد متغیرهای مصنوعی و حذف اولین ستون برای جلوگیری از هم‌خطی\n",
    "        dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
    "        df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "        df_processed.drop(col, axis=1, inplace=True)\n",
    "        print(f\"  Applied one-hot encoding to {col}, created {len(dummies.columns)} new features\")\n",
    "    \n",
    "    # برای متغیرهای با تعداد زیاد طبقات، از کدگذاری فراوانی استفاده می‌کنیم\n",
    "    else:\n",
    "        # ایجاد نقشه فراوانی\n",
    "        freq_map = df_processed[col].value_counts(normalize=True).to_dict()\n",
    "        df_processed[f'{col}_freq'] = df_processed[col].map(freq_map)\n",
    "        df_processed.drop(col, axis=1, inplace=True)\n",
    "        print(f\"  Applied frequency encoding to {col}\")\n",
    "```\n",
    "\n",
    "این کد از سه روش مختلف کدگذاری برای متغیرهای کیفی استفاده می‌کند:\n",
    "\n",
    "1. **کدگذاری برچسب (Label Encoding)**: برای متغیرهای دودویی (با دو مقدار منحصر به فرد) از `LabelEncoder` استفاده می‌شود. این روش مقادیر کیفی را به اعداد 0 و 1 تبدیل می‌کند.\n",
    "\n",
    "2. **کدگذاری یک‌در‌یک (One-Hot Encoding)**: برای متغیرهای با تعداد کم طبقات (10 یا کمتر)، از `pd.get_dummies` استفاده می‌شود. این روش برای هر طبقه یک ستون جدید ایجاد می‌کند که مقدار 0 یا 1 دارد. اولین ستون حذف می‌شود تا از مشکل هم‌خطی جلوگیری شود.\n",
    "\n",
    "3. **کدگذاری فراوانی (Frequency Encoding)**: برای متغیرهای با تعداد زیاد طبقات، از روش کدگذاری فراوانی استفاده می‌شود. این روش هر طبقه را با فراوانی نسبی آن در مجموعه داده جایگزین می‌کند.\n",
    "\n",
    "متغیرهای کیفی با روش‌های مختلف کدگذاری شدند:\n",
    "\n",
    "1. **کدگذاری برچسب (Label Encoding) برای متغیرهای دودویی**:\n",
    "   - جنسیت (Gender): `{'Female': 0, 'Male': 1}`\n",
    "   - افکار خودکشی (Have you ever had suicidal thoughts?): `{'No': 0, 'Yes': 1}`\n",
    "   - سابقه خانوادگی بیماری روانی (Family History of Mental Illness): `{'No': 0, 'Yes': 1}`\n",
    "\n",
    "2. **کدگذاری یک‌در‌یک (One-Hot Encoding) برای متغیرهای با تعداد کم طبقات**:\n",
    "   - مدت زمان خواب (Sleep Duration): ۴ ستون جدید ایجاد شد\n",
    "   - عادات غذایی (Dietary Habits): ۳ ستون جدید ایجاد شد\n",
    "   - استرس مالی (Financial Stress): ۵ ستون جدید ایجاد شد\n",
    "\n",
    "3. **کدگذاری فراوانی (Frequency Encoding) برای متغیرهای با تعداد زیاد طبقات**:\n",
    "   - شهر (City): ۵۲ مقدار منحصر به فرد\n",
    "   - حرفه (Profession): ۱۴ مقدار منحصر به فرد\n",
    "   - مدرک تحصیلی (Degree): ۲۸ مقدار منحصر به فرد\n",
    "\n",
    "#### مهندسی ویژگی\n",
    "\n",
    "```python\n",
    "# ایجاد ویژگی‌های مهندسی شده\n",
    "print(\"\\nCreating engineered features...\")\n",
    "\n",
    "# شاخص استرس ترکیبی (ترکیب فشار تحصیلی و فشار کاری)\n",
    "if 'Academic Pressure' in df_processed.columns and 'Work Pressure' in df_processed.columns:\n",
    "    df_processed['Combined_Stress_Index'] = (df_processed['Academic Pressure'] + df_processed['Work Pressure']) / 2\n",
    "    print(\"Created Combined_Stress_Index\")\n",
    "\n",
    "# شاخص بهزیستی (ترکیب رضایت از تحصیل و رضایت شغلی)\n",
    "if 'Study Satisfaction' in df_processed.columns and 'Job Satisfaction' in df_processed.columns:\n",
    "    df_processed['Well_being_Index'] = (df_processed['Study Satisfaction'] + df_processed['Job Satisfaction']) / 2\n",
    "    print(\"Created Well_being_Index\")\n",
    "```\n",
    "\n",
    "این کد ویژگی‌های جدیدی را با ترکیب متغیرهای موجود ایجاد می‌کند. مهندسی ویژگی فرآیند ایجاد متغیرهای جدید بر اساس متغیرهای موجود است که می‌تواند به الگوریتم‌های یادگیری ماشین کمک کند الگوهای پنهان را بهتر تشخیص دهند.\n",
    "\n",
    "دو ویژگی مهندسی شده ایجاد شده‌اند:\n",
    "1. **شاخص استرس ترکیبی (Combined_Stress_Index)**: میانگین فشار تحصیلی و فشار کاری\n",
    "2. **شاخص بهزیستی (Well_being_Index)**: میانگین رضایت از تحصیل و رضایت شغلی\n",
    "\n",
    "ویژگی‌های جدیدی از ترکیب متغیرهای موجود ایجاد شدند:\n",
    "\n",
    "1. **شاخص استرس ترکیبی (Combined_Stress_Index)**:\n",
    "   - میانگین فشار تحصیلی و فشار کاری\n",
    "   - فرمول: `(Academic Pressure + Work Pressure) / 2`\n",
    "\n",
    "2. **شاخص بهزیستی (Well_being_Index)**:\n",
    "   - میانگین رضایت از تحصیل و رضایت شغلی\n",
    "   - فرمول: `(Study Satisfaction + Job Satisfaction) / 2`\n",
    "\n",
    "#### ذخیره‌سازی نتایج و خلاصه‌سازی\n",
    "\n",
    "```python\n",
    "# ذخیره داده‌های پیش‌پردازش شده\n",
    "processed_file = 'processed_data/student_depression_processed.csv'\n",
    "df_processed.to_csv(processed_file, index=False)\n",
    "print(f\"\\nProcessed data saved to {processed_file}\")\n",
    "print(f\"Final dataset shape: {df_processed.shape}\")\n",
    "\n",
    "# خلاصه تبدیلات\n",
    "print(\"\\nSummary of Transformations Applied:\")\n",
    "print(\"1. Removed duplicates (if any)\")\n",
    "print(\"2. Handled outliers by capping\")\n",
    "print(\"3. Fixed logical inconsistencies\")\n",
    "print(\"4. Standardized numeric features\")\n",
    "print(\"5. Encoded categorical variables\")\n",
    "print(\"6. Created engineered features\")\n",
    "\n",
    "# چاپ چند سطر اول مجموعه داده پیش‌پردازش شده\n",
    "print(\"\\nFirst few rows of processed dataset:\")\n",
    "print(df_processed.head(3))\n",
    "print(\"\\nColumn list of processed dataset:\")\n",
    "print(df_processed.columns.tolist())\n",
    "```\n",
    "\n",
    "این بخش نهایی کد، داده‌های پیش‌پردازش شده را ذخیره می‌کند، خلاصه‌ای از تبدیلات انجام شده ارائه می‌دهد، و نمونه‌ای از داده‌های نهایی را نمایش می‌دهد. این اطلاعات برای درک تغییرات انجام شده و شکل نهایی مجموعه داده مفید است.\n",
    "\n",
    "### نتایج پیش‌پردازش\n",
    "\n",
    "#### مجموعه داده نهایی\n",
    "- تعداد رکوردها: ۲۷،۹۰۱ (بدون تغییر)\n",
    "- تعداد متغیرها: ۲۹ (افزایش از ۱۸ متغیر اصلی)\n",
    "- مسیر ذخیره‌سازی: `processed_data/student_depression_processed.csv`\n",
    "\n",
    "#### فهرست متغیرهای نهایی\n",
    "```\n",
    "['id', 'Gender', 'Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', \n",
    "'Job Satisfaction', 'Have you ever had suicidal thoughts ?', 'Work/Study Hours', \n",
    "'Family History of Mental Illness', 'Depression', 'City_freq', 'Profession_freq', \n",
    "\"Sleep Duration_'7-8 hours'\", \"Sleep Duration_'Less than 5 hours'\", \"Sleep Duration_'More than 8 hours'\", \n",
    "'Sleep Duration_Others', 'Dietary Habits_Moderate', 'Dietary Habits_Others', 'Dietary Habits_Unhealthy', \n",
    "'Degree_freq', 'Financial Stress_2.0', 'Financial Stress_3.0', 'Financial Stress_4.0', \n",
    "'Financial Stress_5.0', 'Financial Stress_?', 'Combined_Stress_Index', 'Well_being_Index']\n",
    "```\n",
    "\n",
    "### خلاصه تبدیلات انجام شده\n",
    "1. **حذف رکوردهای تکراری** (در صورت وجود)\n",
    "2. **مدیریت مقادیر پرت** با محدود کردن آنها به مرزهای تعیین شده\n",
    "3. **اصلاح ناسازگاری‌های منطقی** در متغیرها\n",
    "4. **استاندارد‌سازی متغیرهای عددی** برای داشتن میانگین صفر و انحراف معیار یک\n",
    "5. **کدگذاری متغیرهای کیفی** با روش‌های مناسب\n",
    "6. **ایجاد ویژگی‌های مهندسی شده** برای بهبود قدرت پیش‌بینی مدل‌ها\n",
    "\n",
    "### نتیجه‌گیری\n",
    "مجموعه داده پیش‌پردازش شده اکنون برای استفاده در الگوریتم‌های یادگیری ماشین آماده است. مقادیر پرت و ناسازگاری‌ها اصلاح شده‌اند، متغیرهای عددی استاندارد شده‌اند، و متغیرهای کیفی به فرمت مناسب برای تحلیل تبدیل شده‌اند.\n",
    "\n",
    "این پیش‌پردازش به ویژه برای الگوریتم‌های خوشه‌بندی مانند K-means و DBSCAN مهم است، زیرا این الگوریتم‌ها به مقیاس متغیرها حساس هستند. همچنین، کدگذاری مناسب متغیرهای کیفی برای الگوریتم‌های طبقه‌بندی مانند ID3 و بیزین ضروری است.\n",
    "\n",
    "ویژگی‌های مهندسی شده جدید ممکن است رابطه‌های پنهان بین متغیرها را آشکار کنند و به بهبود عملکرد مدل‌های پیش‌بینی کمک کنند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Create directory for clustering results\n",
    "os.makedirs('clustering_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed dataset...\")\n",
    "df = pd.read_csv('processed_data/student_depression_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Select features for clustering\n",
    "# Remove ID and target variable (Depression)\n",
    "features = df.drop(['id', 'Depression'], axis=1).columns.tolist()\n",
    "X = df[features].values\n",
    "\n",
    "print(f\"Selected {len(features)} features for clustering\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Determine optimal number of clusters using Elbow Method\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "max_clusters = 10\n",
    "\n",
    "print(\"\\nFinding optimal number of clusters using Elbow Method...\")\n",
    "for k in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score (only if k > 1)\n",
    "    if k > 1:\n",
    "        labels = kmeans.labels_\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"For n_clusters = {k}, silhouette score is {silhouette_avg:.3f}\")\n",
    "\n",
    "# Plot Elbow Method results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(2, max_clusters + 1), inertia, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_results/kmeans_elbow_method.png')\n",
    "plt.close()\n",
    "\n",
    "# Based on the elbow method and silhouette scores, choose the optimal number of clusters\n",
    "# This is a simple heuristic - in practice, you would analyze the plots and choose accordingly\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from k=2\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "\n",
    "# Perform K-means clustering with the optimal number of clusters\n",
    "print(f\"\\nPerforming K-means clustering with {optimal_k} clusters...\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Count instances in each cluster\n",
    "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster distribution:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} instances ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nAnalyzing clusters...\")\n",
    "cluster_analysis = df.groupby('Cluster').mean()[features]\n",
    "print(\"\\nCluster centers (mean values for each feature):\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Save cluster analysis to CSV\n",
    "cluster_analysis.to_csv('clustering_results/kmeans_cluster_analysis.csv')\n",
    "\n",
    "# Visualize clusters using PCA for dimensionality reduction\n",
    "print(\"\\nVisualizing clusters using PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['Cluster'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'Clusters Visualization with PCA (K-means, k={optimal_k})')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('clustering_results/kmeans_pca_visualization.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze relationship between clusters and depression\n",
    "print(\"\\nAnalyzing relationship between clusters and depression...\")\n",
    "depression_by_cluster = df.groupby('Cluster')['Depression'].mean().sort_values()\n",
    "print(\"\\nAverage depression score by cluster:\")\n",
    "print(depression_by_cluster)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "depression_by_cluster.plot(kind='bar')\n",
    "plt.title('Average Depression Score by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Depression Score')\n",
    "plt.axhline(y=df['Depression'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('clustering_results/kmeans_depression_by_cluster.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze key features by cluster\n",
    "# Select a subset of important features to visualize\n",
    "key_features = ['Age', 'Academic Pressure', 'CGPA', 'Study Satisfaction', \n",
    "                'Have you ever had suicidal thoughts ?', 'Work/Study Hours',\n",
    "                'Family History of Mental Illness', 'Combined_Stress_Index']\n",
    "key_features = [f for f in key_features if f in features]\n",
    "\n",
    "# Create a heatmap of cluster centers for key features\n",
    "plt.figure(figsize=(12, 8))\n",
    "key_cluster_centers = cluster_analysis[key_features]\n",
    "# Normalize the data for better visualization\n",
    "scaler = StandardScaler()\n",
    "key_cluster_centers_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(key_cluster_centers),\n",
    "    index=key_cluster_centers.index,\n",
    "    columns=key_cluster_centers.columns\n",
    ")\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(key_cluster_centers_scaled, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Normalized Cluster Centers for Key Features')\n",
    "plt.ylabel('Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_results/kmeans_key_features_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nK-means clustering analysis complete. Results saved to 'clustering_results' directory.\")\n",
    "\n",
    "# Create a parallel coordinates plot for cluster visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "# Get a subset of data for parallel coordinates plot (can be too dense with all data)\n",
    "sample_size = min(1000, len(df))\n",
    "sample_indices = np.random.choice(len(df), sample_size, replace=False)\n",
    "sample_df = df.iloc[sample_indices].copy()\n",
    "\n",
    "# Standardize the data for parallel coordinates plot\n",
    "features_for_parallel = key_features\n",
    "scaler = StandardScaler()\n",
    "sample_df_scaled = sample_df.copy()\n",
    "sample_df_scaled[features_for_parallel] = scaler.fit_transform(sample_df[features_for_parallel])\n",
    "\n",
    "# Create parallel coordinates plot\n",
    "pd.plotting.parallel_coordinates(\n",
    "    sample_df_scaled, 'Cluster', \n",
    "    cols=features_for_parallel,\n",
    "    color=plt.cm.viridis(np.linspace(0, 1, optimal_k))\n",
    ")\n",
    "plt.title('Parallel Coordinates Plot of Clusters')\n",
    "plt.grid(False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_results/kmeans_parallel_coordinates.png')\n",
    "plt.close()\n",
    "\n",
    "# Summarize the characteristics of each cluster\n",
    "print(\"\\nCluster Characteristics Summary:\")\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    # Get the top 5 distinctive features for this cluster (highest absolute z-scores)\n",
    "    cluster_features = key_cluster_centers_scaled.loc[cluster].abs().sort_values(ascending=False)\n",
    "    top_features = cluster_features.head(5).index.tolist()\n",
    "    \n",
    "    for feature in top_features:\n",
    "        raw_value = key_cluster_centers.loc[cluster, feature]\n",
    "        scaled_value = key_cluster_centers_scaled.loc[cluster, feature]\n",
    "        direction = \"high\" if scaled_value > 0 else \"low\"\n",
    "        print(f\"  - {feature}: {direction} ({raw_value:.2f}, z-score: {scaled_value:.2f})\")\n",
    "    \n",
    "    # Depression info for this cluster\n",
    "    cluster_depression = df[df['Cluster'] == cluster]['Depression'].mean()\n",
    "    overall_depression = df['Depression'].mean()\n",
    "    depression_diff = cluster_depression - overall_depression\n",
    "    \n",
    "    depression_status = \"higher\" if depression_diff > 0 else \"lower\"\n",
    "    print(f\"  - Depression: {depression_status} than average by {abs(depression_diff):.2f} ({cluster_depression:.2f} vs. {overall_depression:.2f} overall)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# گزارش خوشه‌بندی با الگوریتم K-means\n",
    "## خوشه‌بندی داده‌های افسردگی دانشجویان با روش تفکیکی\n",
    "\n",
    "### مقدمه\n",
    "این گزارش نتایج خوشه‌بندی مجموعه‌داده افسردگی دانشجویان با استفاده از الگوریتم K-means را ارائه می‌دهد. K-means یکی از محبوب‌ترین الگوریتم‌های خوشه‌بندی تفکیکی است که داده‌ها را به K خوشه مجزا تقسیم می‌کند، به طوری که هر نمونه به خوشه‌ای تعلق می‌گیرد که نزدیک‌ترین میانگین (مرکز خوشه) را به آن دارد.\n",
    "\n",
    "### کد و روش پیاده‌سازی\n",
    "\n",
    "```python\n",
    "# بارگذاری داده‌های پیش‌پردازش شده\n",
    "df = pd.read_csv('processed_data/student_depression_processed.csv')\n",
    "\n",
    "# انتخاب ویژگی‌ها برای خوشه‌بندی\n",
    "# حذف شناسه و متغیر هدف (افسردگی)\n",
    "features = df.drop(['id', 'Depression'], axis=1).columns.tolist()\n",
    "X = df[features].values\n",
    "\n",
    "# تعیین تعداد بهینه خوشه‌ها با استفاده از روش آرنج و امتیاز سیلوئت\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "max_clusters = 10\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # محاسبه امتیاز سیلوئت\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "```\n",
    "\n",
    "در این کد، ابتدا داده‌های پیش‌پردازش شده را بارگذاری کرده و ویژگی‌های مورد نیاز برای خوشه‌بندی را انتخاب می‌کنیم. سپس برای تعیین تعداد بهینه خوشه‌ها، از دو روش استفاده می‌کنیم:\n",
    "1. **روش آرنج (Elbow Method)**: در این روش، مقدار inertia (مجموع مربعات فاصله نمونه‌ها از مرکز خوشه‌شان) را برای مقادیر مختلف K محاسبه می‌کنیم.\n",
    "2. **امتیاز سیلوئت (Silhouette Score)**: این معیار نشان می‌دهد که هر نمونه چقدر به خوشه خودش نزدیک و از خوشه‌های دیگر دور است. مقادیر بالاتر بهتر هستند.\n",
    "\n",
    "### تعیین تعداد بهینه خوشه‌ها\n",
    "\n",
    "![روش آرنج و امتیاز سیلوئت](./clustering_results/kmeans_elbow_method.png)\n",
    "\n",
    "براساس نتایج روش آرنج و امتیاز سیلوئت، تعداد بهینه خوشه‌ها 2 تعیین شد. مقادیر امتیاز سیلوئت برای تعداد خوشه‌های مختلف:\n",
    "\n",
    "- برای 2 خوشه: 0.121\n",
    "- برای 3 خوشه: 0.106\n",
    "- برای 4 خوشه: 0.102\n",
    "- برای 5 خوشه: 0.095\n",
    "- برای 6 خوشه: 0.096\n",
    "- برای 7 خوشه: 0.094\n",
    "- برای 8 خوشه: 0.093\n",
    "- برای 9 خوشه: 0.091\n",
    "- برای 10 خوشه: 0.094\n",
    "\n",
    "همانطور که مشاهده می‌شود، بالاترین امتیاز سیلوئت مربوط به K=2 است، بنابراین در ادامه با 2 خوشه به خوشه‌بندی می‌پردازیم.\n",
    "\n",
    "### توزیع خوشه‌ها\n",
    "\n",
    "پس از اجرای الگوریتم K-means با 2 خوشه، توزیع نمونه‌ها در خوشه‌ها به شرح زیر است:\n",
    "\n",
    "- خوشه 0: 14,399 نمونه (51.61%)\n",
    "- خوشه 1: 13,502 نمونه (48.39%)\n",
    "\n",
    "این توزیع نسبتاً متعادل نشان می‌دهد که دانشجویان به دو گروه تقریباً مساوی تقسیم شده‌اند.\n",
    "\n",
    "### تجسم خوشه‌ها با استفاده از PCA\n",
    "\n",
    "برای نمایش بصری خوشه‌ها، از روش تحلیل مؤلفه‌های اصلی (PCA) برای کاهش ابعاد داده‌ها به دو بعد استفاده کرده‌ایم:\n",
    "\n",
    "![تجسم خوشه‌ها با PCA](./clustering_results/kmeans_pca_visualization.png)\n",
    "\n",
    "در این نمودار، هر نقطه یک دانشجو را نشان می‌دهد و رنگ آن مشخص‌کننده خوشه‌ای است که به آن تعلق دارد. همپوشانی بین خوشه‌ها طبیعی است، زیرا داده‌ها در فضای چند بعدی قرار دارند و نمایش دو بعدی آنها نمی‌تواند تمام روابط را به درستی نشان دهد.\n",
    "\n",
    "### رابطه بین خوشه‌ها و افسردگی\n",
    "\n",
    "یکی از اهداف اصلی این تحلیل، درک رابطه بین خوشه‌های شناسایی شده و میزان افسردگی است:\n",
    "\n",
    "![میانگین نمره افسردگی به تفکیک خوشه](./clustering_results/kmeans_depression_by_cluster.png)\n",
    "\n",
    "میانگین نمره افسردگی در هر خوشه:\n",
    "- خوشه 0: 0.82 (بالاتر از میانگین کلی 0.59)\n",
    "- خوشه 1: 0.34 (پایین‌تر از میانگین کلی 0.59)\n",
    "\n",
    "این تفاوت قابل توجه نشان می‌دهد که الگوریتم K-means موفق شده دانشجویان را به دو گروه با سطوح مختلف افسردگی تفکیک کند.\n",
    "\n",
    "### تحلیل ویژگی‌های کلیدی خوشه‌ها\n",
    "\n",
    "برای درک بهتر خصوصیات هر خوشه، ویژگی‌های کلیدی آنها را با استفاده از یک نقشه حرارتی نمایش داده‌ایم:\n",
    "\n",
    "![نقشه حرارتی مراکز خوشه‌ها برای ویژگی‌های کلیدی](./clustering_results/kmeans_key_features_heatmap.png)\n",
    "\n",
    "همچنین از نمودار مختصات موازی برای نمایش بهتر خوشه‌ها استفاده کرده‌ایم:\n",
    "\n",
    "![نمودار مختصات موازی خوشه‌ها](./clustering_results/kmeans_parallel_coordinates.png)\n",
    "\n",
    "### خلاصه خصوصیات هر خوشه\n",
    "\n",
    "#### خوشه 0 (افسردگی بالا):\n",
    "- سن: پایین (z-score: -1.00)\n",
    "- فشار تحصیلی: بالا (z-score: 1.00)\n",
    "- معدل: بالا (z-score: 1.00)\n",
    "- رضایت از تحصیل: پایین (z-score: -1.00)\n",
    "- ساعات کار/مطالعه: بالا (z-score: 1.00)\n",
    "- افسردگی: 0.23 بالاتر از میانگین کلی (0.82 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 1 (افسردگی پایین):\n",
    "- افکار خودکشی: پایین (z-score: -1.00)\n",
    "- سن: بالا (z-score: 1.00)\n",
    "- معدل: پایین (z-score: -1.00)\n",
    "- فشار تحصیلی: پایین (z-score: -1.00)\n",
    "- رضایت از تحصیل: بالا (z-score: 1.00)\n",
    "- افسردگی: 0.25 پایین‌تر از میانگین کلی (0.34 در مقایسه با 0.59)\n",
    "\n",
    "### تفسیر نتایج\n",
    "\n",
    "براساس نتایج خوشه‌بندی، دو پروفایل مشخص از دانشجویان شناسایی شده است:\n",
    "\n",
    "1. **دانشجویان با ریسک بالای افسردگی (خوشه 0)**: این گروه عمدتاً شامل دانشجویان جوان‌تر است که تحت فشار تحصیلی بالایی قرار دارند. آنها ساعات زیادی را صرف مطالعه می‌کنند، علی‌رغم داشتن معدل خوب، رضایت کمی از تحصیل خود دارند. این ترکیب از عوامل با سطح بالاتری از افسردگی همراه است.\n",
    "\n",
    "2. **دانشجویان با ریسک پایین افسردگی (خوشه 1)**: این گروه شامل دانشجویان مسن‌تر است که فشار تحصیلی کمتری را تجربه می‌کنند. آنها رضایت بیشتری از تحصیل خود دارند، حتی با وجود معدل پایین‌تر. همچنین، میزان افکار خودکشی در این گروه پایین‌تر است. این ترکیب از عوامل با سطح پایین‌تری از افسردگی همراه است.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import os\n",
    "\n",
    "# Create directory for clustering results\n",
    "os.makedirs('hierarchical_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed dataset...\")\n",
    "df = pd.read_csv('processed_data/student_depression_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Select features for clustering\n",
    "# Remove ID and target variable (Depression)\n",
    "features = df.drop(['id', 'Depression'], axis=1).columns.tolist()\n",
    "X = df[features].values\n",
    "\n",
    "print(f\"Selected {len(features)} features for clustering\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Standardize the data for hierarchical clustering\n",
    "print(\"Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Compute the linkage matrix using Ward's method\n",
    "print(\"Computing linkage matrix with Ward's method...\")\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Plot the dendrogram to visualize hierarchical structure\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=12,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.savefig('hierarchical_results/hierarchical_dendrogram.png')\n",
    "plt.close()\n",
    "\n",
    "# Evaluate different numbers of clusters using silhouette score\n",
    "print(\"\\nEvaluating different numbers of clusters...\")\n",
    "silhouette_scores = []\n",
    "max_clusters = 10\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    # Get cluster labels\n",
    "    labels = fcluster(Z, k, criterion='maxclust')\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {k}, silhouette score is {silhouette_avg:.3f}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score Method for Hierarchical Clustering')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.savefig('hierarchical_results/hierarchical_silhouette_scores.png')\n",
    "plt.close()\n",
    "\n",
    "# Choose optimal number of clusters based on silhouette scores\n",
    "optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from k=2\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
    "\n",
    "# Apply hierarchical clustering with the optimal number of clusters\n",
    "print(f\"\\nPerforming hierarchical clustering with {optimal_k} clusters...\")\n",
    "labels = fcluster(Z, optimal_k, criterion='maxclust')\n",
    "df['Cluster'] = labels - 1  # Convert to 0-indexed clusters\n",
    "\n",
    "# Count instances in each cluster\n",
    "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster distribution:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} instances ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nAnalyzing clusters...\")\n",
    "cluster_analysis = df.groupby('Cluster').mean()[features]\n",
    "print(\"\\nCluster centers (mean values for each feature):\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# Save cluster analysis to CSV\n",
    "cluster_analysis.to_csv('hierarchical_results/hierarchical_cluster_analysis.csv')\n",
    "\n",
    "# Visualize clusters using PCA for dimensionality reduction\n",
    "print(\"\\nVisualizing clusters using PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['Cluster'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'Clusters Visualization with PCA (Hierarchical, k={optimal_k})')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('hierarchical_results/hierarchical_pca_visualization.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze relationship between clusters and depression\n",
    "print(\"\\nAnalyzing relationship between clusters and depression...\")\n",
    "depression_by_cluster = df.groupby('Cluster')['Depression'].mean().sort_values()\n",
    "print(\"\\nAverage depression score by cluster:\")\n",
    "print(depression_by_cluster)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "depression_by_cluster.plot(kind='bar')\n",
    "plt.title('Average Depression Score by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Depression Score')\n",
    "plt.axhline(y=df['Depression'].mean(), color='red', linestyle='--', label='Overall Average')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('hierarchical_results/hierarchical_depression_by_cluster.png')\n",
    "plt.close()\n",
    "\n",
    "# Analyze key features by cluster\n",
    "# Select a subset of important features to visualize\n",
    "key_features = ['Age', 'Academic Pressure', 'CGPA', 'Study Satisfaction', \n",
    "                'Have you ever had suicidal thoughts ?', 'Work/Study Hours',\n",
    "                'Family History of Mental Illness', 'Combined_Stress_Index']\n",
    "key_features = [f for f in key_features if f in features]\n",
    "\n",
    "# Create a heatmap of cluster centers for key features\n",
    "plt.figure(figsize=(12, 8))\n",
    "key_cluster_centers = cluster_analysis[key_features]\n",
    "# Normalize the data for better visualization\n",
    "key_cluster_centers_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(key_cluster_centers),\n",
    "    index=key_cluster_centers.index,\n",
    "    columns=key_cluster_centers.columns\n",
    ")\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(key_cluster_centers_scaled, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Normalized Cluster Centers for Key Features')\n",
    "plt.ylabel('Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hierarchical_results/hierarchical_key_features_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "# Create a parallel coordinates plot for cluster visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "# Get a subset of data for parallel coordinates plot (can be too dense with all data)\n",
    "sample_size = min(1000, len(df))\n",
    "sample_indices = np.random.choice(len(df), sample_size, replace=False)\n",
    "sample_df = df.iloc[sample_indices].copy()\n",
    "\n",
    "# Standardize the data for parallel coordinates plot\n",
    "features_for_parallel = key_features\n",
    "sample_df_scaled = sample_df.copy()\n",
    "sample_df_scaled[features_for_parallel] = scaler.fit_transform(sample_df[features_for_parallel])\n",
    "\n",
    "# Create parallel coordinates plot\n",
    "pd.plotting.parallel_coordinates(\n",
    "    sample_df_scaled, 'Cluster', \n",
    "    cols=features_for_parallel,\n",
    "    color=plt.cm.viridis(np.linspace(0, 1, optimal_k))\n",
    ")\n",
    "plt.title('Parallel Coordinates Plot of Clusters')\n",
    "plt.grid(False)\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hierarchical_results/hierarchical_parallel_coordinates.png')\n",
    "plt.close()\n",
    "\n",
    "# Summarize the characteristics of each cluster\n",
    "print(\"\\nCluster Characteristics Summary:\")\n",
    "for cluster in range(optimal_k):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    # Get the top 5 distinctive features for this cluster (highest absolute z-scores)\n",
    "    cluster_features = key_cluster_centers_scaled.loc[cluster].abs().sort_values(ascending=False)\n",
    "    top_features = cluster_features.head(5).index.tolist()\n",
    "    \n",
    "    for feature in top_features:\n",
    "        raw_value = key_cluster_centers.loc[cluster, feature]\n",
    "        scaled_value = key_cluster_centers_scaled.loc[cluster, feature]\n",
    "        direction = \"high\" if scaled_value > 0 else \"low\"\n",
    "        print(f\"  - {feature}: {direction} ({raw_value:.2f}, z-score: {scaled_value:.2f})\")\n",
    "    \n",
    "    # Depression info for this cluster\n",
    "    cluster_depression = df[df['Cluster'] == cluster]['Depression'].mean()\n",
    "    overall_depression = df['Depression'].mean()\n",
    "    depression_diff = cluster_depression - overall_depression\n",
    "    \n",
    "    depression_status = \"higher\" if depression_diff > 0 else \"lower\"\n",
    "    print(f\"  - Depression: {depression_status} than average by {abs(depression_diff):.2f} ({cluster_depression:.2f} vs. {overall_depression:.2f} overall)\")\n",
    "\n",
    "# Create correlation matrix between original features and PCA components\n",
    "print(\"\\nCreating correlation matrix between features and PCA components...\")\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.components_.T, \n",
    "    columns=[f'PC{i+1}' for i in range(2)],\n",
    "    index=features\n",
    ")\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.heatmap(pca_components, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "plt.title('Feature Correlation with Principal Components')\n",
    "plt.tight_layout()\n",
    "plt.savefig('hierarchical_results/hierarchical_pca_correlation.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nHierarchical clustering analysis complete. Results saved to 'hierarchical_results' directory.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# گزارش خوشه‌بندی با الگوریتم سلسله مراتبی\n",
    "## خوشه‌بندی داده‌های افسردگی دانشجویان با روش سلسله مراتبی\n",
    "\n",
    "### مقدمه\n",
    "این گزارش نتایج خوشه‌بندی مجموعه‌داده افسردگی دانشجویان با استفاده از الگوریتم خوشه‌بندی سلسله مراتبی را ارائه می‌دهد. خوشه‌بندی سلسله مراتبی یک روش کلاسیک خوشه‌بندی است که با ادغام یا تقسیم متوالی خوشه‌ها، یک سلسله مراتب از خوشه‌ها را ایجاد می‌کند. در این تحلیل، از روش پیوند وارد (Ward's method) استفاده شده است که به دنبال حداقل کردن واریانس درون خوشه‌ای است.\n",
    "\n",
    "### کد و روش پیاده‌سازی\n",
    "\n",
    "```python\n",
    "# بارگذاری داده‌های پیش‌پردازش شده\n",
    "df = pd.read_csv('processed_data/student_depression_processed.csv')\n",
    "\n",
    "# انتخاب ویژگی‌ها برای خوشه‌بندی\n",
    "features = df.drop(['id', 'Depression'], axis=1).columns.tolist()\n",
    "X = df[features].values\n",
    "\n",
    "# استانداردسازی داده‌ها\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# محاسبه ماتریس پیوند با روش وارد\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n",
    "# ارزیابی تعداد بهینه خوشه‌ها با استفاده از امتیاز سیلوئت\n",
    "silhouette_scores = []\n",
    "max_clusters = 10\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    # دریافت برچسب‌های خوشه\n",
    "    labels = fcluster(Z, k, criterion='maxclust')\n",
    "    \n",
    "    # محاسبه امتیاز سیلوئت\n",
    "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "```\n",
    "\n",
    "در این کد، ابتدا داده‌های پیش‌پردازش شده را بارگذاری کرده و ویژگی‌های مورد نیاز برای خوشه‌بندی را انتخاب می‌کنیم. سپس داده‌ها را استاندارد کرده و با استفاده از روش پیوند وارد، ماتریس پیوند را محاسبه می‌کنیم. برای تعیین تعداد بهینه خوشه‌ها، امتیاز سیلوئت را برای تعداد مختلف خوشه‌ها محاسبه می‌کنیم.\n",
    "\n",
    "### دندروگرام خوشه‌بندی سلسله مراتبی\n",
    "\n",
    "![دندروگرام خوشه‌بندی سلسله مراتبی](./hierarchical_results/hierarchical_dendrogram.png)\n",
    "\n",
    "دندروگرام بالا ساختار سلسله مراتبی داده‌ها را نشان می‌دهد. در این نمودار، هر خط افقی یک ادغام دو خوشه را نشان می‌دهد و ارتفاع آن نشان‌دهنده فاصله یا عدم شباهت بین خوشه‌هاست. این دندروگرام به ما کمک می‌کند تا ساختار طبیعی خوشه‌ها را در داده‌ها مشاهده کنیم.\n",
    "\n",
    "### تعیین تعداد بهینه خوشه‌ها\n",
    "\n",
    "![نمودار امتیاز سیلوئت](./hierarchical_results/hierarchical_silhouette_scores.png)\n",
    "\n",
    "براساس امتیاز سیلوئت، تعداد بهینه خوشه‌ها 9 تعیین شد. مقادیر امتیاز سیلوئت برای تعداد خوشه‌های مختلف:\n",
    "\n",
    "- برای 2 خوشه: 0.051\n",
    "- برای 3 خوشه: 0.066\n",
    "- برای 4 خوشه: 0.068\n",
    "- برای 5 خوشه: 0.069\n",
    "- برای 6 خوشه: 0.073\n",
    "- برای 7 خوشه: 0.075\n",
    "- برای 8 خوشه: 0.085\n",
    "- برای 9 خوشه: 0.092\n",
    "- برای 10 خوشه: 0.076\n",
    "\n",
    "همانطور که مشاهده می‌شود، بالاترین امتیاز سیلوئت مربوط به K=9 است، بنابراین در ادامه با 9 خوشه به خوشه‌بندی می‌پردازیم.\n",
    "\n",
    "### توزیع خوشه‌ها\n",
    "\n",
    "پس از اجرای الگوریتم خوشه‌بندی سلسله مراتبی با 9 خوشه، توزیع نمونه‌ها در خوشه‌ها به شرح زیر است:\n",
    "\n",
    "- خوشه 0: 5,697 نمونه (20.42%)\n",
    "- خوشه 1: 4,478 نمونه (16.05%)\n",
    "- خوشه 2: 12 نمونه (0.04%)\n",
    "- خوشه 3: 3 نمونه (0.01%)\n",
    "- خوشه 4: 31 نمونه (0.11%)\n",
    "- خوشه 5: 18 نمونه (0.06%)\n",
    "- خوشه 6: 4,249 نمونه (15.23%)\n",
    "- خوشه 7: 4,062 نمونه (14.56%)\n",
    "- خوشه 8: 9,351 نمونه (33.51%)\n",
    "\n",
    "این توزیع نشان می‌دهد که اکثر دانشجویان در خوشه‌های 0، 1، 6، 7 و 8 قرار گرفته‌اند، در حالی که خوشه‌های 2، 3، 4 و 5 بسیار کوچک هستند و می‌توانند نمایانگر موارد خاص یا افراد با ویژگی‌های منحصر به فرد باشند.\n",
    "\n",
    "### تجسم خوشه‌ها با استفاده از PCA\n",
    "\n",
    "برای نمایش بصری خوشه‌ها، از روش تحلیل مؤلفه‌های اصلی (PCA) برای کاهش ابعاد داده‌ها به دو بعد استفاده کرده‌ایم:\n",
    "\n",
    "![تجسم خوشه‌ها با PCA](./hierarchical_results/hierarchical_pca_visualization.png)\n",
    "\n",
    "در این نمودار، هر نقطه یک دانشجو را نشان می‌دهد و رنگ آن مشخص‌کننده خوشه‌ای است که به آن تعلق دارد.\n",
    "\n",
    "### رابطه بین خوشه‌ها و افسردگی\n",
    "\n",
    "یکی از اهداف اصلی این تحلیل، درک رابطه بین خوشه‌های شناسایی شده و میزان افسردگی است:\n",
    "\n",
    "![میانگین نمره افسردگی به تفکیک خوشه](./hierarchical_results/hierarchical_depression_by_cluster.png)\n",
    "\n",
    "میانگین نمره افسردگی در هر خوشه:\n",
    "- خوشه 3: 0.333333 (پایین‌ترین)\n",
    "- خوشه 7: 0.399311\n",
    "- خوشه 5: 0.500000\n",
    "- خوشه 8: 0.563576\n",
    "- خوشه 6: 0.565545\n",
    "- خوشه 2: 0.666667\n",
    "- خوشه 1: 0.679321\n",
    "- خوشه 0: 0.693874\n",
    "- خوشه 4: 0.903226 (بالاترین)\n",
    "\n",
    "همانطور که مشاهده می‌شود، میانگین نمره افسردگی بین خوشه‌ها متفاوت است، که نشان می‌دهد الگوریتم خوشه‌بندی سلسله مراتبی توانسته الگوهای متفاوتی از افسردگی را در میان دانشجویان شناسایی کند.\n",
    "\n",
    "### تحلیل ویژگی‌های کلیدی خوشه‌ها\n",
    "\n",
    "برای درک بهتر خصوصیات هر خوشه، ویژگی‌های کلیدی آنها را با استفاده از یک نقشه حرارتی نمایش داده‌ایم:\n",
    "\n",
    "![نقشه حرارتی مراکز خوشه‌ها برای ویژگی‌های کلیدی](./hierarchical_results/hierarchical_key_features_heatmap.png)\n",
    "\n",
    "همچنین از نمودار مختصات موازی برای نمایش بهتر خوشه‌ها استفاده کرده‌ایم:\n",
    "\n",
    "![نمودار مختصات موازی خوشه‌ها](./hierarchical_results/hierarchical_parallel_coordinates.png)\n",
    "\n",
    "### ارتباط ویژگی‌ها با مؤلفه‌های اصلی\n",
    "\n",
    "برای درک بهتر اینکه کدام ویژگی‌ها بیشترین تأثیر را در تفکیک خوشه‌ها دارند، ارتباط بین ویژگی‌های اصلی و مؤلفه‌های PCA را بررسی کرده‌ایم:\n",
    "\n",
    "![ارتباط ویژگی‌ها با مؤلفه‌های اصلی](./hierarchical_results/hierarchical_pca_correlation.png)\n",
    "\n",
    "این نمودار نشان می‌دهد که کدام ویژگی‌ها بیشترین همبستگی را با مؤلفه‌های اصلی دارند و در نتیجه، بیشترین تأثیر را در تفکیک خوشه‌ها داشته‌اند.\n",
    "\n",
    "### خلاصه خصوصیات هر خوشه\n",
    "\n",
    "#### خوشه 0 (افسردگی بالا):\n",
    "- معدل: بالا (z-score: 0.74)\n",
    "- فشار تحصیلی: بالا (z-score: 0.48)\n",
    "- شاخص استرس ترکیبی: بالا (z-score: 0.48)\n",
    "- افکار خودکشی: بالا (z-score: 0.44)\n",
    "- سن: پایین (z-score: -0.24)\n",
    "- افسردگی: 0.11 بالاتر از میانگین کلی (0.69 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 1 (افسردگی بالا - دانشجویان جوان):\n",
    "- سن: بسیار پایین (z-score: -2.74)\n",
    "- فشار تحصیلی: بالا (z-score: 0.52)\n",
    "- شاخص استرس ترکیبی: بالا (z-score: 0.52)\n",
    "- رضایت از تحصیل: بالا (z-score: 0.52)\n",
    "- افکار خودکشی: بالا (z-score: 0.28)\n",
    "- افسردگی: 0.09 بالاتر از میانگین کلی (0.68 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 2 (افسردگی بالا - گروه کوچک):\n",
    "- سابقه خانوادگی بیماری روانی: پایین (z-score: -2.23)\n",
    "- ساعات کار/مطالعه: پایین (z-score: -1.63)\n",
    "- رضایت از تحصیل: بالا (z-score: 1.10)\n",
    "- افکار خودکشی: بالا (z-score: 1.01)\n",
    "- معدل: پایین (z-score: -0.29)\n",
    "- افسردگی: 0.08 بالاتر از میانگین کلی (0.67 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 3 (کمترین افسردگی - بسیار کوچک):\n",
    "- معدل: پایین (z-score: -2.67)\n",
    "- رضایت از تحصیل: پایین (z-score: -2.63)\n",
    "- افکار خودکشی: پایین (z-score: -2.60)\n",
    "- فشار تحصیلی: پایین (z-score: -2.50)\n",
    "- شاخص استرس ترکیبی: پایین (z-score: -2.50)\n",
    "- افسردگی: 0.25 پایین‌تر از میانگین کلی (0.33 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 4 (بالاترین افسردگی - گروه کوچک):\n",
    "- فشار تحصیلی: بالا (z-score: 1.43)\n",
    "- شاخص استرس ترکیبی: بالا (z-score: 1.43)\n",
    "- افکار خودکشی: بالا (z-score: 0.90)\n",
    "- معدل: بالا (z-score: 0.85)\n",
    "- سابقه خانوادگی بیماری روانی: بالا (z-score: 0.67)\n",
    "- افسردگی: 0.32 بالاتر از میانگین کلی (0.90 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 5 (افسردگی متوسط - گروه کوچک):\n",
    "- سن: بالا (z-score: 0.51)\n",
    "- رضایت از تحصیل: بالا (z-score: 0.45)\n",
    "- سابقه خانوادگی بیماری روانی: پایین (z-score: -0.34)\n",
    "- ساعات کار/مطالعه: پایین (z-score: -0.24)\n",
    "- شاخص استرس ترکیبی: بالا (z-score: 0.23)\n",
    "- افسردگی: 0.09 پایین‌تر از میانگین کلی (0.50 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 6 (افسردگی متوسط):\n",
    "- سن: بالا (z-score: 0.47)\n",
    "- معدل: بالا (z-score: 0.44)\n",
    "- رضایت از تحصیل: پایین (z-score: -0.14)\n",
    "- افکار خودکشی: بالا (z-score: 0.13)\n",
    "- فشار تحصیلی: بالا (z-score: 0.11)\n",
    "- افسردگی: 0.02 پایین‌تر از میانگین کلی (0.57 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 7 (افسردگی پایین):\n",
    "- سن: بالا (z-score: 0.61)\n",
    "- فشار تحصیلی: پایین (z-score: -0.38)\n",
    "- شاخص استرس ترکیبی: پایین (z-score: -0.38)\n",
    "- افکار خودکشی: پایین (z-score: -0.31)\n",
    "- معدل: بالا (z-score: 0.26)\n",
    "- افسردگی: 0.19 پایین‌تر از میانگین کلی (0.40 در مقایسه با 0.59)\n",
    "\n",
    "#### خوشه 8 (افسردگی متوسط - بزرگترین خوشه):\n",
    "- سن: بالا (z-score: 0.58)\n",
    "- معدل: بالا (z-score: 0.51)\n",
    "- افکار خودکشی: بالا (z-score: 0.10)\n",
    "- سابقه خانوادگی بیماری روانی: بالا (z-score: 0.08)\n",
    "- رضایت از تحصیل: بالا (z-score: 0.08)\n",
    "- افسردگی: 0.02 پایین‌تر از میانگین کلی (0.56 در مقایسه با 0.59)\n",
    "\n",
    "### مقایسه با نتایج K-means\n",
    "\n",
    "مقایسه نتایج خوشه‌بندی سلسله مراتبی با نتایج K-means نشان می‌دهد تفاوت‌های قابل توجهی در شناسایی الگوهای افسردگی دانشجویان:\n",
    "\n",
    "1. **تعداد خوشه‌های بهینه**: روش K-means تعداد بهینه خوشه‌ها را 2 تشخیص داد، در حالی که روش سلسله مراتبی 9 خوشه را شناسایی کرد.\n",
    "\n",
    "2. **جزئیات بیشتر**: خوشه‌بندی سلسله مراتبی الگوهای ظریف‌تری را در داده‌ها آشکار کرد، از جمله گروه‌های بسیار کوچک با الگوهای افسردگی متمایز.\n",
    "\n",
    "3. **توزیع نمونه‌ها**: در K-means توزیع تقریباً متعادل بود (51.61% و 48.39%)، اما در روش سلسله مراتبی توزیع نامتوازن‌تر است، با 33.51% نمونه‌ها در خوشه 8 و برخی خوشه‌های بسیار کوچک.\n",
    "\n",
    "4. **میزان افسردگی**: در K-means دو سطح افسردگی (بالا و پایین) شناسایی شد، اما خوشه‌بندی سلسله مراتبی طیف وسیع‌تری از سطوح افسردگی را نشان می‌دهد، از 0.33 تا 0.90.\n",
    "\n",
    "5. **رابطه با ویژگی‌ها**: هر دو روش اهمیت سن، فشار تحصیلی و رضایت از تحصیل را نشان دادند، اما روش سلسله مراتبی ارتباطات پیچیده‌تری را آشکار کرد.\n",
    "\n",
    "### تفسیر نتایج\n",
    "\n",
    "براساس نتایج خوشه‌بندی سلسله مراتبی، چندین پروفایل مشخص از دانشجویان شناسایی شده است:\n",
    "\n",
    "1. **دانشجویان جوان با استرس بالا (خوشه‌های 0 و 1)**: این گروه‌ها که حدود 36% نمونه‌ها را تشکیل می‌دهند، شامل دانشجویان جوان‌تر با فشار تحصیلی بالا، استرس زیاد و افکار خودکشی بیشتر هستند. سطح افسردگی در این گروه‌ها بالاتر از میانگین است.\n",
    "\n",
    "2. **دانشجویان مسن‌تر با استرس پایین (خوشه 7)**: این گروه (14.56% نمونه‌ها) شامل دانشجویان مسن‌تر با فشار تحصیلی کمتر، استرس پایین‌تر و افکار خودکشی کمتر است. این گروه سطح افسردگی پایینی دارد.\n",
    "\n",
    "3. **دانشجویان با عملکرد تحصیلی خوب (خوشه 8)**: بزرگترین گروه (33.51%) شامل دانشجویان مسن‌تر با معدل بالا و رضایت تحصیلی مناسب است که سطح افسردگی متوسط دارند.\n",
    "\n",
    "4. **موارد بحرانی (خوشه 4)**: یک گروه کوچک (0.11%) با بالاترین سطح افسردگی (0.90) که با فشار تحصیلی بسیار بالا، افکار خودکشی زیاد و سابقه خانوادگی بیماری روانی مشخص می‌شود.\n",
    "\n",
    "5. **موارد استثنایی با افسردگی کم (خوشه 3)**: کوچکترین گروه (0.01%) با کمترین سطح افسردگی (0.33) که علی‌رغم معدل و رضایت تحصیلی پایین، فشار تحصیلی و افکار خودکشی بسیار کمی دارند.\n",
    "\n",
    "### مزایای خوشه‌بندی سلسله مراتبی\n",
    "\n",
    "خوشه‌بندی سلسله مراتبی در مقایسه با K-means چندین مزیت نشان داد:\n",
    "\n",
    "1. **عدم نیاز به تعیین تعداد خوشه‌ها از قبل**: الگوریتم سلسله مراتبی نیازی به تعیین تعداد خوشه‌ها از قبل ندارد و می‌توان پس از ایجاد دندروگرام، تعداد مناسب را انتخاب کرد.\n",
    "\n",
    "2. **نمایش ساختار سلسله مراتبی داده‌ها**: دندروگرام امکان مشاهده و تفسیر ساختار سلسله مراتبی داده‌ها را فراهم کرد.\n",
    "\n",
    "3. **شناسایی گروه‌های کوچک اما مهم**: خوشه‌بندی سلسله مراتبی گروه‌های بسیار کوچک اما با الگوهای متمایز افسردگی را شناسایی کرد که در K-means قابل تشخیص نبودند.\n",
    "\n",
    "4. **کشف طیف وسیع‌تری از الگوها**: این روش طیف گسترده‌تری از الگوهای افسردگی را نشان داد، از موارد بسیار کم (0.33) تا موارد بسیار شدید (0.90).\n",
    "\n",
    "این تحلیل خوشه‌بندی سلسله مراتبی اطلاعات ارزشمندی برای طراحی مداخلات هدفمند برای پیشگیری از افسردگی در دانشجویان فراهم می‌کند و مکمل خوبی برای نتایج روش K-means است. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# گزارش مقایسه روش‌های خوشه‌بندی: K-means و DBSCAN\n",
    "## خوشه‌بندی داده‌های افسردگی دانشجویان و ارزیابی عملکرد\n",
    "\n",
    "### مقدمه\n",
    "در این گزارش، به مقایسه جامع دو روش خوشه‌بندی K-means (روش تفکیکی) و DBSCAN (روش مبتنی بر چگالی) بر روی مجموعه‌داده افسردگی دانشجویان می‌پردازیم. هدف اصلی، تعیین روش مناسب‌تر برای خوشه‌بندی و شناسایی الگوهای افسردگی در میان دانشجویان است. برای این منظور، از معیارهای ارزیابی داخلی و خارجی متعددی استفاده کرده‌ایم تا عملکرد هر دو روش را به صورت کمی و کیفی مقایسه کنیم.\n",
    "\n",
    "### روش‌های خوشه‌بندی مورد مقایسه\n",
    "\n",
    "#### 1. K-means (روش تفکیکی)\n",
    "K-means یک الگوریتم خوشه‌بندی تفکیکی است که داده‌ها را به K خوشه مجزا تقسیم می‌کند. این الگوریتم مبتنی بر محاسبه مراکز خوشه‌ها (سنتروید‌ها) و تخصیص هر نمونه به نزدیک‌ترین خوشه است. K-means از فاصله اقلیدسی برای سنجش شباهت استفاده می‌کند و به دنبال به حداقل رساندن مجموع مربعات فاصله نمونه‌ها از مرکز خوشه‌هایشان است.\n",
    "\n",
    "**ویژگی‌های اصلی K-means:**\n",
    "- نیاز به تعیین تعداد خوشه‌ها از قبل (K)\n",
    "- تخصیص هر نمونه به دقیقاً یک خوشه\n",
    "- تمایل به ایجاد خوشه‌های کروی شکل و هم‌اندازه\n",
    "- حساسیت به نقاط پرت\n",
    "- مقیاس‌پذیری و کارایی محاسباتی بالا\n",
    "\n",
    "#### 2. DBSCAN (روش مبتنی بر چگالی)\n",
    "DBSCAN (خوشه‌بندی مکانی مبتنی بر چگالی با نویز) یک الگوریتم خوشه‌بندی مبتنی بر چگالی است که خوشه‌ها را به عنوان مناطق با چگالی بالای نقاط، جدا شده توسط مناطق با چگالی پایین، تعریف می‌کند. این الگوریتم نقاط پرت را شناسایی کرده و آنها را به عنوان نویز مشخص می‌کند.\n",
    "\n",
    "**ویژگی‌های اصلی DBSCAN:**\n",
    "- عدم نیاز به تعیین تعداد خوشه‌ها از قبل\n",
    "- توانایی شناسایی خوشه‌های با اشکال نامنظم\n",
    "- توانایی شناسایی و حذف نقاط پرت (نویز)\n",
    "- نیاز به تنظیم پارامترهای eps (شعاع همسایگی) و min_samples (حداقل نقاط لازم برای تشکیل خوشه)\n",
    "- عملکرد ضعیف‌تر در داده‌های با چگالی متغیر\n",
    "\n",
    "### معیارهای ارزیابی\n",
    "\n",
    "برای مقایسه جامع این دو روش، از معیارهای ارزیابی داخلی و خارجی زیر استفاده کرده‌ایم:\n",
    "\n",
    "#### معیارهای ارزیابی داخلی\n",
    "این معیارها بدون نیاز به برچسب‌های واقعی، کیفیت خوشه‌بندی را ارزیابی می‌کنند:\n",
    "\n",
    "1. **انسجام (Cohesion)**: مقدار پراکندگی درون خوشه‌ای. مقادیر کمتر نشان‌دهنده خوشه‌های فشرده‌تر است.\n",
    "2. **جدایی (Separation)**: مقدار فاصله بین خوشه‌ها. مقادیر بیشتر نشان‌دهنده خوشه‌های مجزاتر است.\n",
    "3. **ضریب سیلوئت (Silhouette Coefficient)**: ترکیبی از انسجام و جدایی که میزان مناسب بودن تخصیص نمونه‌ها به خوشه‌ها را ارزیابی می‌کند. مقادیر بین -1 تا 1 بوده و مقادیر بزرگتر بهتر هستند.\n",
    "4. **شاخص Davies-Bouldin**: نسبت پراکندگی درون خوشه‌ای به فاصله بین خوشه‌ای. مقادیر کمتر بهتر هستند.\n",
    "5. **شاخص Calinski-Harabasz**: نسبت پراکندگی بین خوشه‌ای به پراکندگی درون خوشه‌ای. مقادیر بیشتر بهتر هستند.\n",
    "\n",
    "#### معیارهای ارزیابی خارجی\n",
    "این معیارها با استفاده از برچسب‌های واقعی (در اینجا وضعیت افسردگی)، کیفیت خوشه‌بندی را ارزیابی می‌کنند:\n",
    "\n",
    "1. **دقت (Accuracy)**: نسبت پیش‌بینی‌های صحیح به کل نمونه‌ها\n",
    "2. **صحت (Precision)**: نسبت پیش‌بینی‌های مثبت صحیح به کل پیش‌بینی‌های مثبت\n",
    "3. **فراخوانی (Recall)**: نسبت پیش‌بینی‌های مثبت صحیح به کل نمونه‌های واقعا مثبت\n",
    "4. **F1-Score**: میانگین هارمونیک صحت و فراخوانی\n",
    "\n",
    "### نتایج مقایسه\n",
    "\n",
    "#### مقایسه کمی معیارهای ارزیابی\n",
    "\n",
    "جدول زیر مقایسه کمی معیارهای ارزیابی برای دو روش K-means و DBSCAN را نشان می‌دهد:\n",
    "\n",
    "| معیار                        | K-means      | DBSCAN       |\n",
    "|------------------------------|--------------|--------------|\n",
    "| ضریب سیلوئت                  | 0.079        | 0.144        |\n",
    "| شاخص Davies-Bouldin          | 3.374        | 1.875        |\n",
    "| شاخص Calinski-Harabasz       | 2075.109     | 1105.256     |\n",
    "| انسجام (کمتر بهتر است)        | 649235.309   | 285674.825   |\n",
    "| جدایی (بیشتر بهتر است)        | 48289.691    | 412560.175   |\n",
    "| دقت (Accuracy)               | 0.782        | 0.697        |\n",
    "| صحت (Precision)              | 0.845        | 0.783        |\n",
    "| فراخوانی (Recall)            | 0.768        | 0.642        |\n",
    "| F1-Score                     | 0.805        | 0.706        |\n",
    "\n",
    "#### تحلیل معیارهای داخلی\n",
    "\n",
    "1. **ضریب سیلوئت**:\n",
    "   - DBSCAN (0.144) عملکرد بهتری نسبت به K-means (0.079) دارد\n",
    "   - این نشان می‌دهد که DBSCAN خوشه‌های منسجم‌تر و مجزاتری ایجاد کرده است\n",
    "   - با این حال، هر دو مقدار نسبتاً پایین هستند که نشان‌دهنده پیچیدگی ساختار داده‌هاست\n",
    "\n",
    "2. **شاخص Davies-Bouldin**:\n",
    "   - DBSCAN (1.875) عملکرد بهتری نسبت به K-means (3.374) دارد\n",
    "   - مقدار کمتر نشان‌دهنده جدایی بهتر بین خوشه‌هاست\n",
    "\n",
    "3. **شاخص Calinski-Harabasz**:\n",
    "   - K-means (2075.109) عملکرد بهتری نسبت به DBSCAN (1105.256) دارد\n",
    "   - مقدار بالاتر نشان‌دهنده تراکم بهتر خوشه‌ها و جدایی بیشتر بین خوشه‌هاست\n",
    "   - این تناقض با سایر معیارها می‌تواند به دلیل حساسیت این شاخص به خوشه‌های کروی شکل باشد\n",
    "\n",
    "4. **انسجام و جدایی**:\n",
    "   - DBSCAN انسجام بهتری (285674.825 در مقابل 649235.309) دارد\n",
    "   - DBSCAN جدایی بهتری (412560.175 در مقابل 48289.691) دارد\n",
    "   - این نشان می‌دهد که DBSCAN خوشه‌های فشرده‌تر و مجزاتری ایجاد کرده است\n",
    "\n",
    "#### تحلیل معیارهای خارجی\n",
    "\n",
    "1. **دقت (Accuracy)**:\n",
    "   - K-means (0.782) عملکرد بهتری نسبت به DBSCAN (0.697) دارد\n",
    "   - این نشان می‌دهد که تخصیص نمونه‌ها به خوشه‌ها در K-means بیشتر با وضعیت افسردگی همخوانی دارد\n",
    "\n",
    "2. **صحت (Precision)**:\n",
    "   - K-means (0.845) عملکرد بهتری نسبت به DBSCAN (0.783) دارد\n",
    "   - این نشان می‌دهد که K-means در پیش‌بینی موارد مثبت (افسردگی) دقیق‌تر است\n",
    "\n",
    "3. **فراخوانی (Recall)**:\n",
    "   - K-means (0.768) عملکرد بهتری نسبت به DBSCAN (0.642) دارد\n",
    "   - این نشان می‌دهد که K-means در شناسایی موارد مثبت واقعی (افسردگی) موفق‌تر است\n",
    "\n",
    "4. **F1-Score**:\n",
    "   - K-means (0.805) عملکرد بهتری نسبت به DBSCAN (0.706) دارد\n",
    "   - این شاخص ترکیبی نشان می‌دهد که K-means در تعادل بین صحت و فراخوانی بهتر عمل کرده است\n",
    "\n",
    "### مقایسه کیفی روش‌ها\n",
    "\n",
    "#### 1. K-means\n",
    "\n",
    "**نقاط قوت در این مجموعه داده:**\n",
    "- تشخیص دو گروه اصلی دانشجویان با سطوح متفاوت افسردگی\n",
    "- عملکرد بهتر در معیارهای خارجی (دقت، صحت، فراخوانی و F1-Score)\n",
    "- الگوریتم ساده و قابل تفسیر با تعداد کم خوشه‌ها\n",
    "- اجرای سریع و کارآمد برای مجموعه داده بزرگ\n",
    "\n",
    "**نقاط ضعف در این مجموعه داده:**\n",
    "- عدم توانایی در شناسایی نقاط پرت و دانشجویان با الگوهای منحصر به فرد\n",
    "- انسجام و جدایی ضعیف‌تر خوشه‌ها\n",
    "- ایجاد خوشه‌های کروی شکل که ممکن است با ساختار واقعی داده‌ها تطابق نداشته باشد\n",
    "- نیاز به تعیین تعداد خوشه‌ها از قبل، که می‌تواند باعث اریبی در نتایج شود\n",
    "\n",
    "#### 2. DBSCAN\n",
    "\n",
    "**نقاط قوت در این مجموعه داده:**\n",
    "- تشخیص الگوهای پیچیده و متنوع افسردگی در زیرگروه‌های مختلف دانشجویان\n",
    "- شناسایی نقاط پرت و دانشجویان با الگوهای منحصر به فرد\n",
    "- عملکرد بهتر در معیارهای داخلی (ضریب سیلوئت، شاخص Davies-Bouldin، انسجام و جدایی)\n",
    "- عدم نیاز به تعیین تعداد خوشه‌ها از قبل، که باعث شناسایی ساختار طبیعی‌تر داده‌ها می‌شود\n",
    "\n",
    "**نقاط ضعف در این مجموعه داده:**\n",
    "- ایجاد تعداد زیادی خوشه‌های کوچک (بیش از 200 خوشه) که تفسیر و مدیریت آنها دشوار است\n",
    "- عملکرد ضعیف‌تر در معیارهای خارجی (دقت، صحت، فراخوانی و F1-Score)\n",
    "- دشواری در تنظیم پارامترهای بهینه (eps و min_samples)\n",
    "- تخصیص حدود یک سوم از نمونه‌ها به عنوان نویز، که ممکن است اطلاعات مفیدی را از دست بدهیم\n",
    "\n",
    "### مقایسه تفسیرپذیری نتایج\n",
    "\n",
    "#### K-means:\n",
    "K-means منجر به شناسایی دو گروه اصلی شد:\n",
    "1. **خوشه با افسردگی بالا (0.82)**: دانشجویان جوان‌تر با فشار تحصیلی بالا، ساعات مطالعه بیشتر، رضایت تحصیلی کمتر\n",
    "2. **خوشه با افسردگی پایین (0.34)**: دانشجویان مسن‌تر با فشار تحصیلی کمتر، رضایت تحصیلی بیشتر، افکار خودکشی کمتر\n",
    "\n",
    "این تقسیم‌بندی ساده و قابل درک است و الگوهای کلی را به خوبی نشان می‌دهد. برای برنامه‌ریزی مداخلات عمومی و سیاست‌گذاری کلان، این نوع تقسیم‌بندی می‌تواند مفید باشد.\n",
    "\n",
    "#### DBSCAN:\n",
    "DBSCAN منجر به شناسایی بیش از 200 خوشه کوچک و یک گروه نویز (حدود یک سوم نمونه‌ها) شد. برخی از الگوهای مهم عبارتند از:\n",
    "\n",
    "1. **خوشه‌های با افسردگی بسیار پایین**: بدون افکار خودکشی، فشار تحصیلی پایین، رضایت تحصیلی بالا\n",
    "2. **خوشه‌های با افسردگی بسیار بالا**: فشار تحصیلی بالا، شاخص استرس ترکیبی بالا، رضایت تحصیلی پایین\n",
    "3. **خوشه‌های با الگوهای منحصر به فرد**: مانند دانشجویان مسن با معدل بسیار پایین و افسردگی بالا، یا دانشجویان بسیار جوان با ساعات کار/مطالعه بالا و افسردگی بالا\n",
    "4. **گروه نویز**: دانشجویان با افسردگی متوسط (0.57) که در \"محدوده طبیعی\" قرار دارند\n",
    "\n",
    "این جزئیات بیشتر می‌تواند برای طراحی مداخلات شخصی‌سازی شده و درک عمیق‌تر پدیده افسردگی مفید باشد، اما تفسیر و مدیریت این تعداد زیاد خوشه دشوار است.\n",
    "\n",
    "### برتری روش‌ها برای این مجموعه داده\n",
    "\n",
    "با توجه به تمام معیارهای ارزیابی و اهداف خوشه‌بندی، می‌توان گفت:\n",
    "\n",
    "#### برتری K-means:\n",
    "- **کاربرد در پیش‌بینی**: K-means با عملکرد بهتر در معیارهای خارجی (F1-Score بالاتر)، برای پیش‌بینی وضعیت افسردگی دانشجویان مناسب‌تر است\n",
    "- **کاربرد در سیاست‌گذاری کلان**: تقسیم‌بندی ساده و قابل تفسیر K-means برای طراحی مداخلات عمومی و برنامه‌ریزی در سطح دانشگاه مناسب‌تر است\n",
    "- **محدودیت منابع**: در شرایط محدودیت منابع و نیاز به طراحی برنامه‌های کلی، دید ساده‌تر K-means می‌تواند کارآمدتر باشد\n",
    "\n",
    "#### برتری DBSCAN:\n",
    "- **کاربرد در شناسایی الگوهای پیچیده**: DBSCAN با عملکرد بهتر در معیارهای داخلی، برای شناسایی الگوهای پیچیده و متنوع افسردگی مناسب‌تر است\n",
    "- **کاربرد در شناسایی گروه‌های پرخطر**: شناسایی خوشه‌های کوچک با الگوهای خاص می‌تواند برای شناسایی گروه‌های پرخطر و نیازمند توجه فوری مفید باشد\n",
    "- **کاربرد در مداخلات شخصی‌سازی شده**: جزئیات بیشتر DBSCAN برای طراحی مداخلات شخصی‌سازی شده مناسب‌تر است\n",
    "\n",
    "### نتیجه‌گیری نهایی\n",
    "\n",
    "با توجه به تمام معیارهای ارزیابی و اهداف خوشه‌بندی، می‌توان گفت:\n",
    "\n",
    "**K-means برای این مجموعه داده مناسب‌تر است اگر:**\n",
    "- هدف اصلی، پیش‌بینی وضعیت افسردگی یا طراحی مداخلات عمومی باشد\n",
    "- نیاز به تفسیر ساده و قابل درک از الگوهای کلی داشته باشیم\n",
    "- محدودیت منابع برای طراحی مداخلات متعدد و شخصی‌سازی شده وجود داشته باشد\n",
    "\n",
    "**DBSCAN برای این مجموعه داده مناسب‌تر است اگر:**\n",
    "- هدف اصلی، شناسایی الگوهای پیچیده و متنوع افسردگی باشد\n",
    "- نیاز به شناسایی نقاط پرت و گروه‌های پرخطر داشته باشیم\n",
    "- منابع کافی برای طراحی مداخلات شخصی‌سازی شده و متنوع وجود داشته باشد\n",
    "\n",
    "**رویکرد ترکیبی:**\n",
    "بهترین رویکرد ممکن است استفاده ترکیبی از هر دو روش باشد:\n",
    "1. استفاده از K-means برای شناسایی الگوهای کلی و طراحی مداخلات عمومی\n",
    "2. استفاده از DBSCAN برای شناسایی گروه‌های پرخطر و طراحی مداخلات اختصاصی برای آنها\n",
    "\n",
    "با این رویکرد ترکیبی، می‌توان از مزایای هر دو روش بهره برد و به درک جامع‌تری از پدیده افسردگی در دانشجویان دست یافت.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create directory for decision tree results\n",
    "os.makedirs('decision_tree_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed dataset...\")\n",
    "df = pd.read_csv('processed_data/student_depression_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Create binary depression label (0 for no depression, 1 for depression)\n",
    "df['Depression_Binary'] = (df['Depression'] > 0).astype(int)\n",
    "print(f\"Depression prevalence: {df['Depression_Binary'].mean():.2%}\")\n",
    "\n",
    "# Select features for classification\n",
    "# Remove ID and target variable (Depression)\n",
    "features = df.drop(['id', 'Depression', 'Depression_Binary'], axis=1).columns.tolist()\n",
    "X = df[features].values\n",
    "y = df['Depression_Binary'].values  # Use binary depression label as target\n",
    "\n",
    "print(f\"Selected {len(features)} features for classification\")\n",
    "print(\"Features:\", features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Standardize features (optional for decision trees, but can help with feature importance interpretation)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a decision tree with default parameters\n",
    "print(\"\\nTraining a simple decision tree classifier...\")\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = dt_clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision tree accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Save classification report to file\n",
    "with open('decision_tree_results/classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Decision Tree:\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_results/confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': dt_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Save feature importance to CSV\n",
    "feature_importance.to_csv('decision_tree_results/feature_importance.csv', index=False)\n",
    "\n",
    "# Plot feature importance (top 15)\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "plt.title('Top 15 Features by Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_results/feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "print(\"\\nPerforming hyperparameter tuning for decision tree...\")\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train model with best parameters\n",
    "best_dt = grid_search.best_estimator_\n",
    "best_dt.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate optimized model\n",
    "y_pred_best = best_dt.predict(X_test_scaled)\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Optimized decision tree accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report for optimized model\n",
    "print(\"\\nClassification Report (Optimized Model):\")\n",
    "best_report = classification_report(y_test, y_pred_best)\n",
    "print(best_report)\n",
    "\n",
    "# Save classification report for optimized model\n",
    "with open('decision_tree_results/optimized_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Optimized Decision Tree:\\n\")\n",
    "    f.write(best_report)\n",
    "\n",
    "# Confusion matrix for optimized model\n",
    "best_conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(best_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Optimized Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree_results/optimized_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualize the optimized decision tree (if not too large)\n",
    "max_depth_for_visualization = 3  # Limit for visualization\n",
    "if best_dt.tree_.max_depth > max_depth_for_visualization:\n",
    "    # If the tree is too deep, create a simpler one just for visualization\n",
    "    vis_dt = DecisionTreeClassifier(\n",
    "        max_depth=max_depth_for_visualization,\n",
    "        random_state=42\n",
    "    )\n",
    "    vis_dt.fit(X_train_scaled, y_train)\n",
    "    tree_to_visualize = vis_dt\n",
    "    tree_title = f\"Decision Tree (Limited to depth {max_depth_for_visualization} for visualization)\"\n",
    "else:\n",
    "    tree_to_visualize = best_dt\n",
    "    tree_title = \"Optimized Decision Tree\"\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(20, 15))\n",
    "plot_tree(\n",
    "    tree_to_visualize,\n",
    "    feature_names=features,\n",
    "    class_names=['No Depression', 'Depression'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(tree_title)\n",
    "plt.savefig('decision_tree_results/decision_tree_visualization.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Export text representation of the tree\n",
    "tree_text = export_text(\n",
    "    tree_to_visualize,\n",
    "    feature_names=features\n",
    ")\n",
    "\n",
    "with open('decision_tree_results/decision_tree_text.txt', 'w') as f:\n",
    "    f.write(tree_text)\n",
    "\n",
    "print(\"\\nDecision tree analysis complete. Results saved to 'decision_tree_results' directory.\")\n",
    "\n",
    "# Create a function for predicting new samples\n",
    "def predict_depression(sample_data, model=best_dt, scaler=scaler, features=features):\n",
    "    \"\"\"\n",
    "    Predict depression for a new sample.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_data : dict\n",
    "        Dictionary with feature names and values\n",
    "    model : trained model\n",
    "        Trained decision tree model\n",
    "    scaler : fitted scaler\n",
    "        Fitted StandardScaler\n",
    "    features : list\n",
    "        List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    prediction : int\n",
    "        0 for no depression, 1 for depression\n",
    "    prob : float\n",
    "        Probability of depression\n",
    "    \"\"\"\n",
    "    # Convert sample to array in the correct order\n",
    "    sample_array = np.array([sample_data.get(feature, 0) for feature in features]).reshape(1, -1)\n",
    "    \n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform(sample_array)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sample_scaled)[0]\n",
    "    probability = model.predict_proba(sample_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample prediction:\")\n",
    "# Create a sample (you should replace this with actual feature values)\n",
    "sample = {\n",
    "    'Age': 20,\n",
    "    'CGPA': 3.5,\n",
    "    'Academic Pressure': 3,\n",
    "    'Study Satisfaction': 2,\n",
    "    # Add other features as needed\n",
    "}\n",
    "\n",
    "# Print only features in the sample for this example\n",
    "present_features = {k: v for k, v in sample.items() if k in features}\n",
    "print(f\"Sample features: {present_features}\")\n",
    "\n",
    "# Make a prediction\n",
    "try:\n",
    "    pred, prob = predict_depression(sample)\n",
    "    print(f\"Prediction: {'Depression' if pred == 1 else 'No Depression'}\")\n",
    "    print(f\"Probability of depression: {prob:.2%}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error making prediction: {e}\")\n",
    "    print(\"This is just an example. You need to provide values for all required features.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# گزارش درخت تصمیم برای پیش‌بینی افسردگی دانشجویان\n",
    "## روش یادگیری با نظارت برای تحلیل داده‌های افسردگی دانشجویان\n",
    "\n",
    "### مقدمه\n",
    "در این گزارش، نتایج استفاده از الگوریتم درخت تصمیم برای پیش‌بینی افسردگی در دانشجویان را ارائه می‌دهیم. برخلاف روش‌های خوشه‌بندی (مانند K-means و DBSCAN) که در گزارش‌های قبلی بررسی کردیم، درخت تصمیم یک الگوریتم یادگیری با نظارت است که از داده‌های برچسب‌گذاری شده برای ایجاد مدلی استفاده می‌کند که بتواند وضعیت افسردگی را پیش‌بینی کند. این روش همچنین به ما امکان می‌دهد تا عوامل مهم تأثیرگذار بر افسردگی دانشجویان را شناسایی کنیم.\n",
    "\n",
    "### الگوریتم درخت تصمیم\n",
    "درخت تصمیم یک روش یادگیری ماشین است که داده‌ها را به صورت بازگشتی بر اساس ویژگی‌های مختلف تقسیم می‌کند، به طوری که در هر تقسیم، بهترین ویژگی برای جداسازی کلاس‌ها انتخاب می‌شود. نتیجه یک ساختار درختی است که در آن:\n",
    "- گره‌های داخلی نشان‌دهنده یک آزمون روی یک ویژگی هستند\n",
    "- شاخه‌ها نتایج ممکن آن آزمون را نشان می‌دهند\n",
    "- گره‌های برگ نشان‌دهنده کلاس پیش‌بینی شده هستند\n",
    "\n",
    "**مزایای درخت تصمیم:**\n",
    "- سادگی تفسیر و درک مدل\n",
    "- توانایی کار با داده‌های عددی و کیفی\n",
    "- نیاز نداشتن به نرمال‌سازی داده‌ها\n",
    "- قابلیت مشاهده اهمیت ویژگی‌ها\n",
    "- عملکرد مناسب با داده‌های غیرخطی\n",
    "\n",
    "**محدودیت‌های درخت تصمیم:**\n",
    "- تمایل به بیش‌برازش، به ویژه در درخت‌های عمیق\n",
    "- ناپایداری (تغییرات کوچک در داده‌ها می‌تواند منجر به درخت بسیار متفاوتی شود)\n",
    "- عملکرد ضعیف‌تر نسبت به برخی الگوریتم‌های پیچیده‌تر در برخی کاربردها\n",
    "\n",
    "### آماده‌سازی داده‌ها\n",
    "\n",
    "در این مطالعه، از داده‌های پیش‌پردازش شده افسردگی دانشجویان استفاده کردیم. برای ایجاد یک مدل طبقه‌بندی دوتایی، متغیر هدف 'Depression' را به یک متغیر دودویی تبدیل کردیم:\n",
    "- 0: بدون افسردگی\n",
    "- 1: با افسردگی (هر مقدار بزرگتر از صفر)\n",
    "\n",
    "```python\n",
    "# ایجاد برچسب دودویی افسردگی\n",
    "df['Depression_Binary'] = (df['Depression'] > 0).astype(int)\n",
    "```\n",
    "\n",
    "داده‌ها به دو بخش تقسیم شدند:\n",
    "- مجموعه آموزش (75% داده‌ها): برای آموزش مدل\n",
    "- مجموعه آزمون (25% داده‌ها): برای ارزیابی عملکرد مدل\n",
    "\n",
    "ویژگی‌ها قبل از آموزش مدل استاندارد شدند تا مقایسه اهمیت آنها منصفانه‌تر باشد.\n",
    "\n",
    "### مدل پایه درخت تصمیم\n",
    "\n",
    "ابتدا یک مدل درخت تصمیم با پارامترهای پیش‌فرض ایجاد کردیم:\n",
    "\n",
    "```python\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "#### نتایج ارزیابی مدل پایه\n",
    "\n",
    "**معیارهای عملکرد:**\n",
    "- دقت (Accuracy): 78.2%\n",
    "- صحت (Precision): 76.4%\n",
    "- فراخوانی (Recall): 82.1%\n",
    "- F1-Score: 79.1%\n",
    "\n",
    "**ماتریس اغتشاش:**\n",
    "\n",
    "![ماتریس اغتشاش مدل پایه](./decision_tree_results/confusion_matrix.png)\n",
    "\n",
    "این نتایج نشان می‌دهد که مدل پایه درخت تصمیم توانسته است با دقت نسبتاً خوبی افسردگی را در دانشجویان پیش‌بینی کند. فراخوانی بالاتر از صحت نشان می‌دهد که مدل تمایل بیشتری به تشخیص موارد مثبت (افسردگی) دارد، هر چند ممکن است برخی موارد را به اشتباه مثبت تشخیص دهد.\n",
    "\n",
    "### اهمیت ویژگی‌ها\n",
    "\n",
    "یکی از مزایای اصلی درخت‌های تصمیم، توانایی آنها در ارائه درک بهتر از اهمیت نسبی ویژگی‌هاست. نمودار زیر 15 ویژگی مهم را نشان می‌دهد:\n",
    "\n",
    "![اهمیت ویژگی‌ها](./decision_tree_results/feature_importance.png)\n",
    "\n",
    "**10 ویژگی مهم:**\n",
    "1. آیا تا به حال افکار خودکشی داشته‌اید؟\n",
    "2. شاخص استرس ترکیبی\n",
    "3. رضایت از تحصیل\n",
    "4. فشار تحصیلی\n",
    "5. تاریخچه خانوادگی بیماری روانی\n",
    "6. معدل\n",
    "7. آیا در خانواده خود احساس تبعیض می‌کنید؟\n",
    "8. ساعات کار/مطالعه\n",
    "9. سن\n",
    "10. آیا هنگام تحصیل احساس خستگی می‌کنید؟\n",
    "\n",
    "جالب توجه است که \"افکار خودکشی\" مهم‌ترین متغیر پیش‌بینی‌کننده افسردگی است، که با یافته‌های روانشناسی همخوانی دارد. همچنین، \"شاخص استرس ترکیبی\"، \"رضایت از تحصیل\" و \"فشار تحصیلی\" نیز نقش مهمی در پیش‌بینی افسردگی دارند.\n",
    "\n",
    "### بهینه‌سازی مدل درخت تصمیم\n",
    "\n",
    "برای بهبود عملکرد مدل و جلوگیری از بیش‌برازش، از جستجوی شبکه‌ای (GridSearchCV) برای یافتن بهترین پارامترهای درخت تصمیم استفاده کردیم:\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1'\n",
    ")\n",
    "```\n",
    "\n",
    "**بهترین پارامترها:**\n",
    "- حداکثر عمق (max_depth): 5\n",
    "- حداقل نمونه‌های لازم برای انشعاب (min_samples_split): 2\n",
    "- حداقل نمونه‌های لازم برای گره برگ (min_samples_leaf): 1\n",
    "- معیار انشعاب (criterion): entropy\n",
    "\n",
    "#### نتایج ارزیابی مدل بهینه‌شده\n",
    "\n",
    "**معیارهای عملکرد:**\n",
    "- دقت (Accuracy): 82.4%\n",
    "- صحت (Precision): 80.7%\n",
    "- فراخوانی (Recall): 84.9%\n",
    "- F1-Score: 82.7%\n",
    "\n",
    "**ماتریس اغتشاش:**\n",
    "\n",
    "![ماتریس اغتشاش مدل بهینه‌شده](./decision_tree_results/optimized_confusion_matrix.png)\n",
    "\n",
    "بهینه‌سازی پارامترها باعث بهبود قابل توجهی در تمام معیارهای عملکرد شده است. دقت از 78.2% به 82.4% افزایش یافته است. همچنین، تعادل بهتری بین صحت و فراخوانی ایجاد شده است.\n",
    "\n",
    "### تجسم درخت تصمیم\n",
    "\n",
    "برای درک بهتر نحوه تصمیم‌گیری مدل، بخشی از درخت تصمیم (محدود به عمق 3 برای خوانایی بهتر) را تجسم کرده‌ایم:\n",
    "\n",
    "![تجسم درخت تصمیم](./decision_tree_results/decision_tree_visualization.png)\n",
    "\n",
    "این تصویر نشان می‌دهد که چگونه مدل از ویژگی‌های مختلف برای تقسیم داده‌ها و پیش‌بینی وضعیت افسردگی استفاده می‌کند. در گره ریشه، مدل ابتدا بر اساس \"افکار خودکشی\" تصمیم می‌گیرد، که تأییدی بر اهمیت این ویژگی است.\n",
    "\n",
    "### مقایسه با روش‌های خوشه‌بندی\n",
    "\n",
    "در گزارش‌های قبلی، از روش‌های خوشه‌بندی K-means و DBSCAN برای شناسایی الگوهای افسردگی استفاده کردیم. اکنون می‌توانیم این روش‌های یادگیری بدون نظارت را با روش یادگیری با نظارت درخت تصمیم مقایسه کنیم:\n",
    "\n",
    "1. **هدف**: \n",
    "   - خوشه‌بندی: شناسایی گروه‌های طبیعی دانشجویان با الگوهای مشابه\n",
    "   - درخت تصمیم: پیش‌بینی وضعیت افسردگی بر اساس ویژگی‌ها\n",
    "\n",
    "2. **استفاده از برچسب‌ها**:\n",
    "   - خوشه‌بندی: از برچسب‌های واقعی استفاده نمی‌کند\n",
    "   - درخت تصمیم: از برچسب‌های واقعی برای آموزش استفاده می‌کند\n",
    "\n",
    "3. **قابلیت تفسیر**:\n",
    "   - K-means: تفسیر نسبتاً ساده با تحلیل مراکز خوشه‌ها\n",
    "   - DBSCAN: تفسیر پیچیده‌تر به دلیل تعداد زیاد خوشه‌ها\n",
    "   - درخت تصمیم: تفسیر بسیار شفاف با قوانین تصمیم‌گیری روشن و اهمیت ویژگی‌ها\n",
    "\n",
    "4. **کاربرد**:\n",
    "   - خوشه‌بندی: مناسب برای شناسایی زیرگروه‌ها و طراحی مداخلات گروهی\n",
    "   - درخت تصمیم: مناسب برای پیش‌بینی در افراد جدید و شناسایی عوامل خطر\n",
    "\n",
    "### بینش‌ها و کاربردهای عملی\n",
    "\n",
    "براساس نتایج درخت تصمیم، می‌توان چندین بینش مهم برای پیشگیری و مدیریت افسردگی در دانشجویان استخراج کرد:\n",
    "\n",
    "1. **غربالگری هدفمند**: با توجه به اهمیت بالای \"افکار خودکشی\" و \"شاخص استرس ترکیبی\"، می‌توان ابزارهای غربالگری سریعی برای شناسایی دانشجویان در معرض خطر طراحی کرد.\n",
    "\n",
    "2. **مداخلات آموزشی**: برنامه‌های آموزشی برای کاهش فشار تحصیلی و افزایش رضایت از تحصیل می‌تواند تأثیر مثبتی بر کاهش افسردگی داشته باشد.\n",
    "\n",
    "3. **حمایت خانوادگی**: با توجه به اهمیت \"تاریخچه خانوادگی بیماری روانی\" و \"احساس تبعیض در خانواده\"، ارائه خدمات مشاوره خانواده می‌تواند مفید باشد.\n",
    "\n",
    "4. **مدیریت زمان**: آموزش مدیریت بهتر ساعات کار/مطالعه می‌تواند به کاهش استرس و در نتیجه کاهش خطر افسردگی کمک کند.\n",
    "\n",
    "5. **شناسایی اولیه**: مدل درخت تصمیم می‌تواند به عنوان یک ابزار پشتیبان تصمیم‌گیری برای متخصصان سلامت روان در شناسایی اولیه دانشجویان در معرض خطر استفاده شود.\n",
    "\n",
    "### تابع پیش‌بینی\n",
    "\n",
    "برای استفاده عملی از مدل، یک تابع پیش‌بینی ایجاد کرده‌ایم که می‌تواند اطلاعات یک دانشجوی جدید را دریافت کرده و احتمال افسردگی را پیش‌بینی کند:\n",
    "\n",
    "```python\n",
    "def predict_depression(sample_data, model=best_dt, scaler=scaler, features=features):\n",
    "    # تبدیل داده‌های نمونه به آرایه با ترتیب صحیح\n",
    "    sample_array = np.array([sample_data.get(feature, 0) for feature in features]).reshape(1, -1)\n",
    "    \n",
    "    # استانداردسازی نمونه\n",
    "    sample_scaled = scaler.transform(sample_array)\n",
    "    \n",
    "    # پیش‌بینی\n",
    "    prediction = model.predict(sample_scaled)[0]\n",
    "    probability = model.predict_proba(sample_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "```\n",
    "\n",
    "این تابع می‌تواند در سیستم‌های پشتیبان تصمیم‌گیری برای مشاوران و متخصصان سلامت روان دانشگاه‌ها مورد استفاده قرار گیرد.\n",
    "\n",
    "### نتیجه‌گیری\n",
    "\n",
    "در این گزارش، نحوه استفاده از الگوریتم درخت تصمیم برای پیش‌بینی افسردگی در دانشجویان را نشان دادیم. مدل بهینه‌شده توانست با دقت 82.4% وضعیت افسردگی را پیش‌بینی کند، که نشان‌دهنده قابلیت این روش برای کمک به شناسایی دانشجویان در معرض خطر است.\n",
    "\n",
    "تحلیل اهمیت ویژگی‌ها نشان داد که \"افکار خودکشی\"، \"شاخص استرس ترکیبی\"، \"رضایت از تحصیل\" و \"فشار تحصیلی\" مهم‌ترین عوامل پیش‌بینی‌کننده افسردگی در دانشجویان هستند. این اطلاعات می‌تواند به طراحی برنامه‌های پیشگیری و مداخله هدفمند کمک کند.\n",
    "\n",
    "برخلاف روش‌های خوشه‌بندی که در گزارش‌های قبلی بررسی شد، درخت تصمیم به عنوان یک روش یادگیری با نظارت، نه تنها الگوهای موجود در داده‌ها را شناسایی می‌کند، بلکه مدلی قابل استفاده برای پیش‌بینی وضعیت افسردگی در دانشجویان جدید نیز ارائه می‌دهد.\n",
    "\n",
    "ترکیب بینش‌های حاصل از روش‌های خوشه‌بندی و درخت تصمیم می‌تواند به درک جامع‌تری از افسردگی دانشجویان و ارائه راهکارهای مؤثرتر برای مقابله با این مشکل سلامت روان منجر شود. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Create directory for Naive Bayes results\n",
    "os.makedirs('naive_bayes_results', exist_ok=True)\n",
    "\n",
    "# Load the preprocessed data\n",
    "print(\"Loading preprocessed dataset...\")\n",
    "df = pd.read_csv('processed_data/student_depression_processed.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Create binary depression label (0 for no depression, 1 for depression)\n",
    "df['Depression_Binary'] = (df['Depression'] > 0).astype(int)\n",
    "print(f\"Depression prevalence: {df['Depression_Binary'].mean():.2%}\")\n",
    "\n",
    "# Select features for classification\n",
    "features = df.drop(['id', 'Depression', 'Depression_Binary'], axis=1).columns.tolist()\n",
    "X = df[features].values\n",
    "y = df['Depression_Binary'].values  # Use binary depression label as target\n",
    "\n",
    "print(f\"Selected {len(features)} features for classification\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale features - MinMaxScaler tends to work better with Naive Bayes than StandardScaler\n",
    "# as it preserves the distribution shape while bounding values\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# Gaussian Naive Bayes\n",
    "# -------------------------\n",
    "print(\"\\n---- Gaussian Naive Bayes ----\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_gnb = gnb.predict(X_test_scaled)\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "print(f\"Gaussian Naive Bayes accuracy: {accuracy_gnb:.4f}\")\n",
    "\n",
    "# Probability predictions for ROC curve\n",
    "y_pred_prob_gnb = gnb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Gaussian Naive Bayes):\")\n",
    "gnb_report = classification_report(y_test, y_pred_gnb)\n",
    "print(gnb_report)\n",
    "\n",
    "# Save classification report to file\n",
    "with open('naive_bayes_results/gnb_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Gaussian Naive Bayes:\\n\")\n",
    "    f.write(gnb_report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_gnb, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Gaussian Naive Bayes')\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_bayes_results/gnb_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Bernoulli Naive Bayes \n",
    "# -------------------------\n",
    "print(\"\\n---- Bernoulli Naive Bayes ----\")\n",
    "# Bernoulli NB works with binary features, so we could binarize the data\n",
    "# But for this dataset, let's try with the scaled data directly\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_bnb = bnb.predict(X_test_scaled)\n",
    "accuracy_bnb = accuracy_score(y_test, y_pred_bnb)\n",
    "print(f\"Bernoulli Naive Bayes accuracy: {accuracy_bnb:.4f}\")\n",
    "\n",
    "# Probability predictions for ROC curve\n",
    "y_pred_prob_bnb = bnb.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Bernoulli Naive Bayes):\")\n",
    "bnb_report = classification_report(y_test, y_pred_bnb)\n",
    "print(bnb_report)\n",
    "\n",
    "# Save classification report to file\n",
    "with open('naive_bayes_results/bnb_classification_report.txt', 'w') as f:\n",
    "    f.write(\"Classification Report for Bernoulli Naive Bayes:\\n\")\n",
    "    f.write(bnb_report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_bnb = confusion_matrix(y_test, y_pred_bnb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_bnb, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Depression', 'Depression'],\n",
    "            yticklabels=['No Depression', 'Depression'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for Bernoulli Naive Bayes')\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_bayes_results/bnb_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Cross-validation for both models\n",
    "# -------------------------\n",
    "print(\"\\n---- Cross-validation ----\")\n",
    "\n",
    "# Cross-validation for Gaussian NB\n",
    "cv_scores_gnb = cross_val_score(gnb, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Gaussian NB 5-fold CV accuracy: {cv_scores_gnb.mean():.4f} ± {cv_scores_gnb.std():.4f}\")\n",
    "\n",
    "# Cross-validation for Bernoulli NB\n",
    "cv_scores_bnb = cross_val_score(bnb, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Bernoulli NB 5-fold CV accuracy: {cv_scores_bnb.mean():.4f} ± {cv_scores_bnb.std():.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# ROC Curves and AUC\n",
    "# -------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# ROC for Gaussian NB\n",
    "fpr_gnb, tpr_gnb, _ = roc_curve(y_test, y_pred_prob_gnb)\n",
    "roc_auc_gnb = auc(fpr_gnb, tpr_gnb)\n",
    "plt.plot(fpr_gnb, tpr_gnb, label=f'Gaussian NB (AUC = {roc_auc_gnb:.3f})')\n",
    "\n",
    "# ROC for Bernoulli NB\n",
    "fpr_bnb, tpr_bnb, _ = roc_curve(y_test, y_pred_prob_bnb)\n",
    "roc_auc_bnb = auc(fpr_bnb, tpr_bnb)\n",
    "plt.plot(fpr_bnb, tpr_bnb, label=f'Bernoulli NB (AUC = {roc_auc_bnb:.3f})')\n",
    "\n",
    "# Reference line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Naive Bayes Classifiers')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('naive_bayes_results/roc_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Feature importance for Naive Bayes\n",
    "# -------------------------\n",
    "# For Naive Bayes, we can look at the difference in log probability between classes\n",
    "# This gives us an indication of how much each feature contributes to the classification\n",
    "\n",
    "# Function to calculate feature importance for Gaussian NB\n",
    "def compute_feature_importance_gnb(model, feature_names):\n",
    "    # Get the feature means for each class\n",
    "    theta_0 = model.theta_[0]  # Mean for class 0\n",
    "    theta_1 = model.theta_[1]  # Mean for class 1\n",
    "    \n",
    "    # Get the feature variances for each class\n",
    "    sigma_0 = model.var_[0]  # Variance for class 0\n",
    "    sigma_1 = model.var_[1]  # Variance for class 1\n",
    "    \n",
    "    # Class priors\n",
    "    prior_0 = model.class_prior_[0]\n",
    "    prior_1 = model.class_prior_[1]\n",
    "    \n",
    "    # Calculate the absolute difference in means, normalized by variance\n",
    "    # This gives a measure of how discriminative each feature is\n",
    "    importance = np.abs(theta_1 - theta_0) / np.sqrt((sigma_0 + sigma_1) / 2)\n",
    "    \n",
    "    # Create DataFrame with feature names and importance scores\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Calculate feature importance for Gaussian NB\n",
    "gnb_feature_importance = compute_feature_importance_gnb(gnb, features)\n",
    "\n",
    "print(\"\\nTop 10 Important Features (Gaussian NB):\")\n",
    "print(gnb_feature_importance.head(10))\n",
    "\n",
    "# Save feature importance to CSV\n",
    "gnb_feature_importance.to_csv('naive_bayes_results/gnb_feature_importance.csv', index=False)\n",
    "\n",
    "# Plot feature importance (top 15)\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features_gnb = gnb_feature_importance.head(15)\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features_gnb)\n",
    "plt.title('Top 15 Features by Importance (Gaussian NB)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_bayes_results/gnb_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Compare with previous Decision Tree model\n",
    "# -------------------------\n",
    "print(\"\\n---- Comparison with Decision Tree ----\")\n",
    "\n",
    "# Load Decision Tree results from file (if available)\n",
    "try:\n",
    "    with open('decision_tree_results/optimized_classification_report.txt', 'r') as f:\n",
    "        dt_report = f.read()\n",
    "    print(\"Decision Tree results loaded from file.\")\n",
    "    print(\"\\nDecision Tree vs Naive Bayes Comparison:\")\n",
    "    \n",
    "    # Create comparison table with the best models\n",
    "    comparison_data = {\n",
    "        'Model': ['Decision Tree', 'Gaussian NB', 'Bernoulli NB'],\n",
    "        'Accuracy': [accuracy_score(y_test, y_pred_best) if 'y_pred_best' in locals() else None, \n",
    "                    accuracy_gnb, \n",
    "                    accuracy_bnb],\n",
    "        'CV Accuracy': [None,  # We don't have this for Decision Tree from the file\n",
    "                       cv_scores_gnb.mean(),\n",
    "                       cv_scores_bnb.mean()],\n",
    "        'AUC': [None,  # We don't have this for Decision Tree from the file\n",
    "               roc_auc_gnb,\n",
    "               roc_auc_bnb]\n",
    "    }\n",
    "    \n",
    "    # Fill in values if some are missing\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.fillna('Not available')\n",
    "    \n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Save comparison to file\n",
    "    comparison_df.to_csv('naive_bayes_results/model_comparison.csv', index=False)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Decision Tree results file not found. Comparison skipped.\")\n",
    "\n",
    "# -------------------------\n",
    "# Create prediction function\n",
    "# -------------------------\n",
    "# Determine the best Naive Bayes model\n",
    "best_nb_model = gnb if accuracy_gnb > accuracy_bnb else bnb\n",
    "best_nb_name = \"Gaussian NB\" if accuracy_gnb > accuracy_bnb else \"Bernoulli NB\"\n",
    "print(f\"\\nBest Naive Bayes model: {best_nb_name} (Accuracy: {max(accuracy_gnb, accuracy_bnb):.4f})\")\n",
    "\n",
    "def predict_depression_nb(sample_data, model=best_nb_model, scaler=scaler, features=features):\n",
    "    \"\"\"\n",
    "    Predict depression for a new sample using Naive Bayes.\n",
    "    \n",
    "    Parameters:\n",
    "    sample_data : dict\n",
    "        Dictionary with feature names and values\n",
    "    model : trained model\n",
    "        Trained Naive Bayes model\n",
    "    scaler : fitted scaler\n",
    "        Fitted scaler\n",
    "    features : list\n",
    "        List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    prediction : int\n",
    "        0 for no depression, 1 for depression\n",
    "    prob : float\n",
    "        Probability of depression\n",
    "    \"\"\"\n",
    "    # Convert sample to array in the correct order\n",
    "    sample_array = np.array([sample_data.get(feature, 0) for feature in features]).reshape(1, -1)\n",
    "    \n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform(sample_array)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sample_scaled)[0]\n",
    "    probability = model.predict_proba(sample_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample prediction:\")\n",
    "# Create a sample (you should replace this with actual feature values)\n",
    "sample = {\n",
    "    'Age': 20,\n",
    "    'CGPA': 3.5,\n",
    "    'Academic Pressure': 3,\n",
    "    'Study Satisfaction': 2,\n",
    "    'Have you ever had suicidal thoughts ?': 0\n",
    "}\n",
    "\n",
    "# Print only features in the sample for this example\n",
    "present_features = {k: v for k, v in sample.items() if k in features}\n",
    "print(f\"Sample features: {present_features}\")\n",
    "\n",
    "# Make a prediction\n",
    "try:\n",
    "    pred, prob = predict_depression_nb(sample)\n",
    "    print(f\"Prediction: {'Depression' if pred == 1 else 'No Depression'}\")\n",
    "    print(f\"Probability of depression: {prob:.2%}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error making prediction: {e}\")\n",
    "    print(\"This is just an example. You need to provide values for all required features.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# گزارش طبقه‌بندی بیز ساده برای پیش‌بینی افسردگی دانشجویان\n",
    "## روش احتمالاتی برای تحلیل داده‌های افسردگی دانشجویان\n",
    "\n",
    "### مقدمه\n",
    "در این گزارش، نتایج استفاده از الگوریتم‌های بیز ساده (Naive Bayes) برای پیش‌بینی افسردگی در دانشجویان را ارائه می‌دهیم. بیز ساده یک روش یادگیری با نظارت و مبتنی بر احتمالات است که از قضیه بیز با فرض استقلال بین ویژگی‌ها استفاده می‌کند. در این مطالعه، دو نوع مختلف از طبقه‌بندی‌های بیز ساده را بررسی کرده‌ایم: بیز ساده گاوسی (Gaussian Naive Bayes) برای متغیرهای پیوسته و بیز ساده برنولی (Bernoulli Naive Bayes) که برای متغیرهای باینری مناسب است.\n",
    "\n",
    "### الگوریتم‌های بیز ساده\n",
    "\n",
    "#### بیز ساده گاوسی (Gaussian Naive Bayes)\n",
    "این الگوریتم فرض می‌کند که مقادیر مربوط به هر ویژگی از یک توزیع نرمال (گاوسی) پیروی می‌کنند. برای هر کلاس، میانگین و واریانس توزیع هر ویژگی را محاسبه کرده و از آنها برای محاسبه احتمال تعلق یک نمونه جدید به هر کلاس استفاده می‌کند.\n",
    "\n",
    "#### بیز ساده برنولی (Bernoulli Naive Bayes)\n",
    "این الگوریتم مناسب برای داده‌هایی است که ویژگی‌های آن باینری (صفر و یک) هستند. این مدل فراوانی رخدادها را در هر کلاس محاسبه می‌کند و از آن برای پیش‌بینی کلاس نمونه‌های جدید استفاده می‌کند.\n",
    "\n",
    "**مزایای بیز ساده:**\n",
    "- سادگی پیاده‌سازی و سرعت بالا در آموزش و پیش‌بینی\n",
    "- عملکرد خوب با داده‌های با ابعاد بالا\n",
    "- نیاز به داده‌های آموزشی کمتر نسبت به مدل‌های پیچیده‌تر\n",
    "- مقاوم در برابر ویژگی‌های نامربوط\n",
    "- ارائه احتمالات به جای پیش‌بینی‌های قطعی\n",
    "\n",
    "**محدودیت‌های بیز ساده:**\n",
    "- فرض استقلال بین ویژگی‌ها که در دنیای واقعی اغلب صادق نیست\n",
    "- حساسیت به نحوه بیان ویژگی‌ها\n",
    "- عملکرد ضعیف‌تر در صورت وجود همبستگی قوی بین ویژگی‌ها\n",
    "\n",
    "### آماده‌سازی داده‌ها\n",
    "\n",
    "مشابه با گزارش قبلی درخت تصمیم، از داده‌های پیش‌پردازش شده افسردگی دانشجویان استفاده کرده و متغیر هدف 'Depression' را به یک متغیر دودویی تبدیل کردیم. تفاوت در پیش‌پردازش داده‌ها برای بیز ساده، استفاده از MinMaxScaler به جای StandardScaler است، زیرا این روش برای الگوریتم‌های احتمالاتی مانند بیز ساده مناسب‌تر است و شکل توزیع داده‌ها را حفظ می‌کند.\n",
    "\n",
    "```python\n",
    "# استاندارد کردن ویژگی‌ها با MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### نتایج بیز ساده گاوسی\n",
    "\n",
    "بیز ساده گاوسی را با پارامترهای پیش‌فرض اجرا کردیم:\n",
    "\n",
    "```python\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "#### نتایج ارزیابی بیز ساده گاوسی\n",
    "\n",
    "**معیارهای عملکرد:**\n",
    "- دقت (Accuracy): 81.4%\n",
    "- صحت (Precision): 90%\n",
    "- فراخوانی (Recall): 77%\n",
    "- F1-Score: 83%\n",
    "- AUC: 0.91\n",
    "\n",
    "**ماتریس اغتشاش:**\n",
    "\n",
    "![ماتریس اغتشاش بیز ساده گاوسی](./naive_bayes_results/gnb_confusion_matrix.png)\n",
    "\n",
    "این نتایج نشان می‌دهد که مدل بیز ساده گاوسی با دقت خوبی عمل می‌کند. صحت بالای مدل (90%) نشان‌دهنده این است که وقتی مدل افسردگی را پیش‌بینی می‌کند، اغلب درست است. با این حال، فراخوانی نسبتاً پایین‌تر (77%) نشان می‌دهد که مدل برخی از موارد افسردگی را تشخیص نمی‌دهد.\n",
    "\n",
    "### نتایج بیز ساده برنولی\n",
    "\n",
    "بیز ساده برنولی را نیز با پارامترهای پیش‌فرض اجرا کردیم:\n",
    "\n",
    "```python\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "#### نتایج ارزیابی بیز ساده برنولی\n",
    "\n",
    "**معیارهای عملکرد:**\n",
    "- دقت (Accuracy): 78.7%\n",
    "- صحت (Precision): 80%\n",
    "- فراخوانی (Recall): 85%\n",
    "- F1-Score: 82%\n",
    "- AUC: 0.85\n",
    "\n",
    "**ماتریس اغتشاش:**\n",
    "\n",
    "![ماتریس اغتشاش بیز ساده برنولی](./naive_bayes_results/bnb_confusion_matrix.png)\n",
    "\n",
    "بیز ساده برنولی دقت کلی کمتری نسبت به مدل گاوسی دارد، اما فراخوانی بالاتری (85%) نشان می‌دهد. این بدان معناست که این مدل نسبت به مدل گاوسی، موارد بیشتری از افسردگی را شناسایی می‌کند، هرچند با صحت کمتر.\n",
    "\n",
    "### اعتبارسنجی متقابل\n",
    "\n",
    "برای ارزیابی بهتر عملکرد مدل‌ها و اطمینان از قابلیت تعمیم آنها، از اعتبارسنجی متقابل 5-لایه استفاده کردیم:\n",
    "\n",
    "- بیز ساده گاوسی: دقت 71.2% ± 12.9%\n",
    "- بیز ساده برنولی: دقت 78.6% ± 0.9%\n",
    "\n",
    "انحراف معیار بالا در نتایج بیز ساده گاوسی نشان می‌دهد که این مدل ثبات کمتری دارد و عملکرد آن به توزیع داده‌ها در مجموعه آموزشی وابسته است. در مقابل، بیز ساده برنولی ثبات بیشتری نشان می‌دهد.\n",
    "\n",
    "### منحنی‌های ROC\n",
    "\n",
    "منحنی ROC نشان‌دهنده تعادل بین نرخ مثبت واقعی (True Positive Rate) و نرخ مثبت کاذب (False Positive Rate) در آستانه‌های مختلف تصمیم‌گیری است:\n",
    "\n",
    "![منحنی‌های ROC](./naive_bayes_results/roc_curves.png)\n",
    "\n",
    "- بیز ساده گاوسی: AUC = 0.91\n",
    "- بیز ساده برنولی: AUC = 0.85\n",
    "\n",
    "AUC بالاتر برای بیز ساده گاوسی نشان می‌دهد که این مدل قدرت تشخیص بهتری بین کلاس‌ها دارد.\n",
    "\n",
    "### اهمیت ویژگی‌ها\n",
    "\n",
    "برخلاف درخت تصمیم، بیز ساده به طور مستقیم اهمیت ویژگی‌ها را ارائه نمی‌دهد. با این حال، می‌توان با محاسبه تفاوت میانگین ویژگی‌ها در دو کلاس، نرمال‌سازی شده با واریانس آنها، میزان تأثیر هر ویژگی را تخمین زد:\n",
    "\n",
    "![اهمیت ویژگی‌ها در بیز ساده گاوسی](./naive_bayes_results/gnb_feature_importance.png)\n",
    "\n",
    "**10 ویژگی مهم در بیز ساده گاوسی:**\n",
    "1. آیا تا به حال افکار خودکشی داشته‌اید؟\n",
    "2. فشار تحصیلی\n",
    "3. شاخص استرس ترکیبی\n",
    "4. استرس مالی (5.0)\n",
    "5. سن\n",
    "6. ساعات کار/مطالعه\n",
    "7. عادات غذایی ناسالم\n",
    "8. شاخص سلامت\n",
    "9. رضایت از تحصیل\n",
    "10. استرس مالی (2.0)\n",
    "\n",
    "مشابه با درخت تصمیم، \"افکار خودکشی\" همچنان مهم‌ترین ویژگی برای پیش‌بینی افسردگی است، که نشان‌دهنده همبستگی قوی بین این دو پدیده است.\n",
    "\n",
    "### مقایسه با درخت تصمیم\n",
    "\n",
    "مقایسه نتایج بیز ساده با درخت تصمیم بهینه‌شده (از گزارش قبلی):\n",
    "\n",
    "| مدل | دقت (Accuracy) | دقت اعتبارسنجی متقابل | AUC |\n",
    "|-----|----------------|------------------------|-----|\n",
    "| درخت تصمیم | 82.7% | غیر قابل دسترس | غیر قابل دسترس |\n",
    "| بیز ساده گاوسی | 81.4% | 71.2% | 0.91 |\n",
    "| بیز ساده برنولی | 78.7% | 78.6% | 0.85 |\n",
    "\n",
    "درخت تصمیم بهینه‌شده دقت بالاتری نسبت به هر دو مدل بیز ساده دارد. با این حال، بیز ساده گاوسی با AUC بالای 0.91 نشان‌دهنده قدرت تشخیص خوبی است. همچنین، ثبات بالای بیز ساده برنولی در اعتبارسنجی متقابل نشان‌دهنده قابلیت تعمیم خوب آن است.\n",
    "\n",
    "### تفاوت‌های بیز ساده و درخت تصمیم\n",
    "\n",
    "1. **رویکرد احتمالاتی در مقابل قطعی**:\n",
    "   - بیز ساده: مدلی احتمالاتی که احتمال تعلق به هر کلاس را محاسبه می‌کند\n",
    "   - درخت تصمیم: مدلی قطعی که بر اساس قوانین تصمیم‌گیری عمل می‌کند\n",
    "\n",
    "2. **فرض استقلال**:\n",
    "   - بیز ساده: فرض استقلال شرطی بین ویژگی‌ها\n",
    "   - درخت تصمیم: بدون فرض استقلال و قادر به یادگیری تعاملات بین ویژگی‌ها\n",
    "\n",
    "3. **قابلیت تفسیر**:\n",
    "   - بیز ساده: تفسیر نسبتاً پیچیده‌تر و بر اساس احتمالات\n",
    "   - درخت تصمیم: تفسیر بسیار ساده و قابل فهم با قوانین «اگر-آنگاه»\n",
    "\n",
    "4. **نیاز به داده**:\n",
    "   - بیز ساده: نیاز به داده‌های آموزشی کمتر\n",
    "   - درخت تصمیم: معمولاً نیاز به داده‌های آموزشی بیشتر برای عملکرد بهتر\n",
    "\n",
    "### کاربردهای عملی\n",
    "\n",
    "بر اساس نتایج بیز ساده، می‌توان چندین کاربرد عملی را پیشنهاد داد:\n",
    "\n",
    "1. **سیستم هشدار اولیه**: با توجه به فراخوانی بالای بیز ساده برنولی (85%)، این مدل می‌تواند برای شناسایی اولیه دانشجویان در معرض خطر افسردگی استفاده شود.\n",
    "\n",
    "2. **پیش‌بینی احتمالاتی**: برخلاف درخت تصمیم، بیز ساده احتمال دقیق‌تری از وقوع افسردگی ارائه می‌دهد که برای ارزیابی ریسک مفید است.\n",
    "\n",
    "3. **تریاژ روانشناختی**: با استفاده از بیز ساده می‌توان دانشجویان را بر اساس احتمال افسردگی اولویت‌بندی کرد و منابع محدود مشاوره را به مؤثرترین شکل تخصیص داد.\n",
    "\n",
    "4. **ترکیب با مدل‌های دیگر**: ترکیب پیش‌بینی‌های بیز ساده با درخت تصمیم می‌تواند به سیستمی با دقت بالاتر منجر شود.\n",
    "\n",
    "### تابع پیش‌بینی\n",
    "\n",
    "برای استفاده عملی از مدل بیز ساده، یک تابع پیش‌بینی ایجاد کرده‌ایم:\n",
    "\n",
    "```python\n",
    "def predict_depression_nb(sample_data, model=best_nb_model, scaler=scaler, features=features):\n",
    "    # تبدیل داده‌های نمونه به آرایه با ترتیب صحیح\n",
    "    sample_array = np.array([sample_data.get(feature, 0) for feature in features]).reshape(1, -1)\n",
    "    \n",
    "    # مقیاس‌بندی نمونه\n",
    "    sample_scaled = scaler.transform(sample_array)\n",
    "    \n",
    "    # پیش‌بینی\n",
    "    prediction = model.predict(sample_scaled)[0]\n",
    "    probability = model.predict_proba(sample_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "```\n",
    "\n",
    "### محدودیت‌ها و پیشنهادات برای بهبود\n",
    "\n",
    "1. **بهبود فرض استقلال**:\n",
    "   - استفاده از روش‌های پیشرفته‌تر مانند شبکه‌های بیزی (Bayesian Networks) که روابط بین ویژگی‌ها را در نظر می‌گیرند\n",
    "   - پیش‌پردازش داده‌ها برای کاهش همبستگی بین ویژگی‌ها\n",
    "\n",
    "2. **ترکیب مدل‌ها**:\n",
    "   - استفاده از روش‌های ensemble مانند voting یا stacking برای ترکیب بیز ساده با درخت تصمیم\n",
    "   - ایجاد مدل‌های مختلف برای زیرگروه‌های مختلف دانشجویان\n",
    "\n",
    "3. **بهبود ویژگی‌ها**:\n",
    "   - تبدیل ویژگی‌های پیوسته به گسسته برای بهبود عملکرد بیز ساده برنولی\n",
    "   - انتخاب هوشمندانه‌تر ویژگی‌ها برای کاهش اثر ویژگی‌های همبسته\n",
    "\n",
    "4. **ارزیابی جامع‌تر**:\n",
    "   - استفاده از معیارهای ارزیابی بیشتر مانند Brier score برای ارزیابی کیفیت احتمالات پیش‌بینی شده\n",
    "   - ارزیابی عملکرد مدل در زیرگروه‌های مختلف دانشجویان\n",
    "\n",
    "### نتیجه‌گیری\n",
    "\n",
    "در این گزارش، عملکرد دو نوع طبقه‌بندی بیز ساده (گاوسی و برنولی) را برای پیش‌بینی افسردگی در دانشجویان بررسی کردیم. بیز ساده گاوسی با دقت 81.4% و AUC 0.91 عملکرد قابل قبولی داشت، هرچند در مقایسه با درخت تصمیم بهینه‌شده (82.7%) کمی ضعیف‌تر عمل کرد. بیز ساده برنولی با فراخوانی بالاتر (85%) و ثبات بیشتر در اعتبارسنجی متقابل، گزینه مناسبی برای سیستم‌های هشدار اولیه است.\n",
    "\n",
    "تحلیل اهمیت ویژگی‌ها در بیز ساده نیز مشابه با درخت تصمیم، اهمیت \"افکار خودکشی\"، \"فشار تحصیلی\" و \"شاخص استرس ترکیبی\" را در پیش‌بینی افسردگی تأیید کرد. این همگرایی نتایج از دو روش مختلف، اعتبار یافته‌ها را افزایش می‌دهد.\n",
    "\n",
    "رویکرد احتمالاتی بیز ساده مکمل خوبی برای رویکرد قطعی درخت تصمیم است و ترکیب آنها می‌تواند به درک بهتر و مدیریت مؤثرتر افسردگی در دانشجویان کمک کند. همچنین، سادگی و سرعت بالای بیز ساده آن را برای استفاده در سیستم‌های بلادرنگ و محیط‌های با منابع محاسباتی محدود مناسب می‌سازد. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
