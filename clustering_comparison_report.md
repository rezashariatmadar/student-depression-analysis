# گزارش مقایسه روش‌های خوشه‌بندی: K-means و DBSCAN
## خوشه‌بندی داده‌های افسردگی دانشجویان و ارزیابی عملکرد

### مقدمه
در این گزارش، به مقایسه جامع دو روش خوشه‌بندی K-means (روش تفکیکی) و DBSCAN (روش مبتنی بر چگالی) بر روی مجموعه‌داده افسردگی دانشجویان می‌پردازیم. هدف اصلی، تعیین روش مناسب‌تر برای خوشه‌بندی و شناسایی الگوهای افسردگی در میان دانشجویان است. برای این منظور، از معیارهای ارزیابی داخلی و خارجی متعددی استفاده کرده‌ایم تا عملکرد هر دو روش را به صورت کمی و کیفی مقایسه کنیم.

### روش‌های خوشه‌بندی مورد مقایسه

#### 1. K-means (روش تفکیکی)
K-means یک الگوریتم خوشه‌بندی تفکیکی است که داده‌ها را به K خوشه مجزا تقسیم می‌کند. این الگوریتم مبتنی بر محاسبه مراکز خوشه‌ها (سنتروید‌ها) و تخصیص هر نمونه به نزدیک‌ترین خوشه است. K-means از فاصله اقلیدسی برای سنجش شباهت استفاده می‌کند و به دنبال به حداقل رساندن مجموع مربعات فاصله نمونه‌ها از مرکز خوشه‌هایشان است.

**ویژگی‌های اصلی K-means:**
- نیاز به تعیین تعداد خوشه‌ها از قبل (K)
- تخصیص هر نمونه به دقیقاً یک خوشه
- تمایل به ایجاد خوشه‌های کروی شکل و هم‌اندازه
- حساسیت به نقاط پرت
- مقیاس‌پذیری و کارایی محاسباتی بالا

#### 2. DBSCAN (روش مبتنی بر چگالی)
DBSCAN (خوشه‌بندی مکانی مبتنی بر چگالی با نویز) یک الگوریتم خوشه‌بندی مبتنی بر چگالی است که خوشه‌ها را به عنوان مناطق با چگالی بالای نقاط، جدا شده توسط مناطق با چگالی پایین، تعریف می‌کند. این الگوریتم نقاط پرت را شناسایی کرده و آنها را به عنوان نویز مشخص می‌کند.

**ویژگی‌های اصلی DBSCAN:**
- عدم نیاز به تعیین تعداد خوشه‌ها از قبل
- توانایی شناسایی خوشه‌های با اشکال نامنظم
- توانایی شناسایی و حذف نقاط پرت (نویز)
- نیاز به تنظیم پارامترهای eps (شعاع همسایگی) و min_samples (حداقل نقاط لازم برای تشکیل خوشه)
- عملکرد ضعیف‌تر در داده‌های با چگالی متغیر

### معیارهای ارزیابی

برای مقایسه جامع این دو روش، از معیارهای ارزیابی داخلی و خارجی زیر استفاده کرده‌ایم:

#### معیارهای ارزیابی داخلی
این معیارها بدون نیاز به برچسب‌های واقعی، کیفیت خوشه‌بندی را ارزیابی می‌کنند:

1. **انسجام (Cohesion)**: مقدار پراکندگی درون خوشه‌ای. مقادیر کمتر نشان‌دهنده خوشه‌های فشرده‌تر است.
2. **جدایی (Separation)**: مقدار فاصله بین خوشه‌ها. مقادیر بیشتر نشان‌دهنده خوشه‌های مجزاتر است.
3. **ضریب سیلوئت (Silhouette Coefficient)**: ترکیبی از انسجام و جدایی که میزان مناسب بودن تخصیص نمونه‌ها به خوشه‌ها را ارزیابی می‌کند. مقادیر بین -1 تا 1 بوده و مقادیر بزرگتر بهتر هستند.
4. **شاخص Davies-Bouldin**: نسبت پراکندگی درون خوشه‌ای به فاصله بین خوشه‌ای. مقادیر کمتر بهتر هستند.
5. **شاخص Calinski-Harabasz**: نسبت پراکندگی بین خوشه‌ای به پراکندگی درون خوشه‌ای. مقادیر بیشتر بهتر هستند.

#### معیارهای ارزیابی خارجی
این معیارها با استفاده از برچسب‌های واقعی (در اینجا وضعیت افسردگی)، کیفیت خوشه‌بندی را ارزیابی می‌کنند:

1. **دقت (Accuracy)**: نسبت پیش‌بینی‌های صحیح به کل نمونه‌ها
2. **صحت (Precision)**: نسبت پیش‌بینی‌های مثبت صحیح به کل پیش‌بینی‌های مثبت
3. **فراخوانی (Recall)**: نسبت پیش‌بینی‌های مثبت صحیح به کل نمونه‌های واقعا مثبت
4. **F1-Score**: میانگین هارمونیک صحت و فراخوانی

### نتایج مقایسه

#### مقایسه کمی معیارهای ارزیابی

جدول زیر مقایسه کمی معیارهای ارزیابی برای دو روش K-means و DBSCAN را نشان می‌دهد:

| معیار                        | K-means      | DBSCAN       |
|------------------------------|--------------|--------------|
| ضریب سیلوئت                  | 0.079        | 0.144        |
| شاخص Davies-Bouldin          | 3.374        | 1.875        |
| شاخص Calinski-Harabasz       | 2075.109     | 1105.256     |
| انسجام (کمتر بهتر است)        | 649235.309   | 285674.825   |
| جدایی (بیشتر بهتر است)        | 48289.691    | 412560.175   |
| دقت (Accuracy)               | 0.782        | 0.697        |
| صحت (Precision)              | 0.845        | 0.783        |
| فراخوانی (Recall)            | 0.768        | 0.642        |
| F1-Score                     | 0.805        | 0.706        |

#### تحلیل معیارهای داخلی

1. **ضریب سیلوئت**:
   - DBSCAN (0.144) عملکرد بهتری نسبت به K-means (0.079) دارد
   - این نشان می‌دهد که DBSCAN خوشه‌های منسجم‌تر و مجزاتری ایجاد کرده است
   - با این حال، هر دو مقدار نسبتاً پایین هستند که نشان‌دهنده پیچیدگی ساختار داده‌هاست

2. **شاخص Davies-Bouldin**:
   - DBSCAN (1.875) عملکرد بهتری نسبت به K-means (3.374) دارد
   - مقدار کمتر نشان‌دهنده جدایی بهتر بین خوشه‌هاست

3. **شاخص Calinski-Harabasz**:
   - K-means (2075.109) عملکرد بهتری نسبت به DBSCAN (1105.256) دارد
   - مقدار بالاتر نشان‌دهنده تراکم بهتر خوشه‌ها و جدایی بیشتر بین خوشه‌هاست
   - این تناقض با سایر معیارها می‌تواند به دلیل حساسیت این شاخص به خوشه‌های کروی شکل باشد

4. **انسجام و جدایی**:
   - DBSCAN انسجام بهتری (285674.825 در مقابل 649235.309) دارد
   - DBSCAN جدایی بهتری (412560.175 در مقابل 48289.691) دارد
   - این نشان می‌دهد که DBSCAN خوشه‌های فشرده‌تر و مجزاتری ایجاد کرده است

#### تحلیل معیارهای خارجی

1. **دقت (Accuracy)**:
   - K-means (0.782) عملکرد بهتری نسبت به DBSCAN (0.697) دارد
   - این نشان می‌دهد که تخصیص نمونه‌ها به خوشه‌ها در K-means بیشتر با وضعیت افسردگی همخوانی دارد

2. **صحت (Precision)**:
   - K-means (0.845) عملکرد بهتری نسبت به DBSCAN (0.783) دارد
   - این نشان می‌دهد که K-means در پیش‌بینی موارد مثبت (افسردگی) دقیق‌تر است

3. **فراخوانی (Recall)**:
   - K-means (0.768) عملکرد بهتری نسبت به DBSCAN (0.642) دارد
   - این نشان می‌دهد که K-means در شناسایی موارد مثبت واقعی (افسردگی) موفق‌تر است

4. **F1-Score**:
   - K-means (0.805) عملکرد بهتری نسبت به DBSCAN (0.706) دارد
   - این شاخص ترکیبی نشان می‌دهد که K-means در تعادل بین صحت و فراخوانی بهتر عمل کرده است

### مقایسه کیفی روش‌ها

#### 1. K-means

**نقاط قوت در این مجموعه داده:**
- تشخیص دو گروه اصلی دانشجویان با سطوح متفاوت افسردگی
- عملکرد بهتر در معیارهای خارجی (دقت، صحت، فراخوانی و F1-Score)
- الگوریتم ساده و قابل تفسیر با تعداد کم خوشه‌ها
- اجرای سریع و کارآمد برای مجموعه داده بزرگ

**نقاط ضعف در این مجموعه داده:**
- عدم توانایی در شناسایی نقاط پرت و دانشجویان با الگوهای منحصر به فرد
- انسجام و جدایی ضعیف‌تر خوشه‌ها
- ایجاد خوشه‌های کروی شکل که ممکن است با ساختار واقعی داده‌ها تطابق نداشته باشد
- نیاز به تعیین تعداد خوشه‌ها از قبل، که می‌تواند باعث اریبی در نتایج شود

#### 2. DBSCAN

**نقاط قوت در این مجموعه داده:**
- تشخیص الگوهای پیچیده و متنوع افسردگی در زیرگروه‌های مختلف دانشجویان
- شناسایی نقاط پرت و دانشجویان با الگوهای منحصر به فرد
- عملکرد بهتر در معیارهای داخلی (ضریب سیلوئت، شاخص Davies-Bouldin، انسجام و جدایی)
- عدم نیاز به تعیین تعداد خوشه‌ها از قبل، که باعث شناسایی ساختار طبیعی‌تر داده‌ها می‌شود

**نقاط ضعف در این مجموعه داده:**
- ایجاد تعداد زیادی خوشه‌های کوچک (بیش از 200 خوشه) که تفسیر و مدیریت آنها دشوار است
- عملکرد ضعیف‌تر در معیارهای خارجی (دقت، صحت، فراخوانی و F1-Score)
- دشواری در تنظیم پارامترهای بهینه (eps و min_samples)
- تخصیص حدود یک سوم از نمونه‌ها به عنوان نویز، که ممکن است اطلاعات مفیدی را از دست بدهیم

### مقایسه تفسیرپذیری نتایج

#### K-means:
K-means منجر به شناسایی دو گروه اصلی شد:
1. **خوشه با افسردگی بالا (0.82)**: دانشجویان جوان‌تر با فشار تحصیلی بالا، ساعات مطالعه بیشتر، رضایت تحصیلی کمتر
2. **خوشه با افسردگی پایین (0.34)**: دانشجویان مسن‌تر با فشار تحصیلی کمتر، رضایت تحصیلی بیشتر، افکار خودکشی کمتر

این تقسیم‌بندی ساده و قابل درک است و الگوهای کلی را به خوبی نشان می‌دهد. برای برنامه‌ریزی مداخلات عمومی و سیاست‌گذاری کلان، این نوع تقسیم‌بندی می‌تواند مفید باشد.

#### DBSCAN:
DBSCAN منجر به شناسایی بیش از 200 خوشه کوچک و یک گروه نویز (حدود یک سوم نمونه‌ها) شد. برخی از الگوهای مهم عبارتند از:

1. **خوشه‌های با افسردگی بسیار پایین**: بدون افکار خودکشی، فشار تحصیلی پایین، رضایت تحصیلی بالا
2. **خوشه‌های با افسردگی بسیار بالا**: فشار تحصیلی بالا، شاخص استرس ترکیبی بالا، رضایت تحصیلی پایین
3. **خوشه‌های با الگوهای منحصر به فرد**: مانند دانشجویان مسن با معدل بسیار پایین و افسردگی بالا، یا دانشجویان بسیار جوان با ساعات کار/مطالعه بالا و افسردگی بالا
4. **گروه نویز**: دانشجویان با افسردگی متوسط (0.57) که در "محدوده طبیعی" قرار دارند

این جزئیات بیشتر می‌تواند برای طراحی مداخلات شخصی‌سازی شده و درک عمیق‌تر پدیده افسردگی مفید باشد، اما تفسیر و مدیریت این تعداد زیاد خوشه دشوار است.

### برتری روش‌ها برای این مجموعه داده

با توجه به تمام معیارهای ارزیابی و اهداف خوشه‌بندی، می‌توان گفت:

#### برتری K-means:
- **کاربرد در پیش‌بینی**: K-means با عملکرد بهتر در معیارهای خارجی (F1-Score بالاتر)، برای پیش‌بینی وضعیت افسردگی دانشجویان مناسب‌تر است
- **کاربرد در سیاست‌گذاری کلان**: تقسیم‌بندی ساده و قابل تفسیر K-means برای طراحی مداخلات عمومی و برنامه‌ریزی در سطح دانشگاه مناسب‌تر است
- **محدودیت منابع**: در شرایط محدودیت منابع و نیاز به طراحی برنامه‌های کلی، دید ساده‌تر K-means می‌تواند کارآمدتر باشد

#### برتری DBSCAN:
- **کاربرد در شناسایی الگوهای پیچیده**: DBSCAN با عملکرد بهتر در معیارهای داخلی، برای شناسایی الگوهای پیچیده و متنوع افسردگی مناسب‌تر است
- **کاربرد در شناسایی گروه‌های پرخطر**: شناسایی خوشه‌های کوچک با الگوهای خاص می‌تواند برای شناسایی گروه‌های پرخطر و نیازمند توجه فوری مفید باشد
- **کاربرد در مداخلات شخصی‌سازی شده**: جزئیات بیشتر DBSCAN برای طراحی مداخلات شخصی‌سازی شده مناسب‌تر است

### نتیجه‌گیری نهایی

با توجه به تمام معیارهای ارزیابی و اهداف خوشه‌بندی، می‌توان گفت:

**K-means برای این مجموعه داده مناسب‌تر است اگر:**
- هدف اصلی، پیش‌بینی وضعیت افسردگی یا طراحی مداخلات عمومی باشد
- نیاز به تفسیر ساده و قابل درک از الگوهای کلی داشته باشیم
- محدودیت منابع برای طراحی مداخلات متعدد و شخصی‌سازی شده وجود داشته باشد

**DBSCAN برای این مجموعه داده مناسب‌تر است اگر:**
- هدف اصلی، شناسایی الگوهای پیچیده و متنوع افسردگی باشد
- نیاز به شناسایی نقاط پرت و گروه‌های پرخطر داشته باشیم
- منابع کافی برای طراحی مداخلات شخصی‌سازی شده و متنوع وجود داشته باشد

**رویکرد ترکیبی:**
بهترین رویکرد ممکن است استفاده ترکیبی از هر دو روش باشد:
1. استفاده از K-means برای شناسایی الگوهای کلی و طراحی مداخلات عمومی
2. استفاده از DBSCAN برای شناسایی گروه‌های پرخطر و طراحی مداخلات اختصاصی برای آنها

با این رویکرد ترکیبی، می‌توان از مزایای هر دو روش بهره برد و به درک جامع‌تری از پدیده افسردگی در دانشجویان دست یافت.

### پیشنهادات برای بهبود

1. **بهبود K-means**:
   - استفاده از K-means++ برای مقداردهی اولیه بهتر مراکز خوشه‌ها
   - آزمایش با تعداد خوشه‌های بیشتر برای شناسایی الگوهای ظریف‌تر
   - ترکیب با روش‌های کاهش ابعاد برای بهبود عملکرد در فضای با ابعاد بالا

2. **بهبود DBSCAN**:
   - استفاده از OPTICS یا HDBSCAN برای مدیریت بهتر چگالی‌های متغیر و کاهش تعداد پارامترها
   - خوشه‌بندی چندسطحی برای مدیریت تعداد زیاد خوشه‌ها
   - بررسی جداگانه نقاط نویز برای یافتن الگوهای پنهان

3. **رویکردهای ترکیبی**:
   - استفاده از DBSCAN برای حذف نقاط پرت، سپس اعمال K-means روی داده‌های پاکسازی شده
   - خوشه‌بندی سلسله مراتبی برای یافتن ساختار کلی، سپس اعمال K-means یا DBSCAN در هر خوشه اصلی

### منابع و مراجع

1. MacQueen, J. (1967). "Some methods for classification and analysis of multivariate observations." Proceedings of the fifth Berkeley symposium on mathematical statistics and probability.
2. Ester, M., Kriegel, H. P., Sander, J., & Xu, X. (1996). "A density-based algorithm for discovering clusters in large spatial databases with noise." In KDD.
3. Rousseeuw, P. J. (1987). "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis." Journal of computational and applied mathematics.
4. Davies, D. L., & Bouldin, D. W. (1979). "A cluster separation measure." IEEE transactions on pattern analysis and machine intelligence.
5. Calinski, T., & Harabasz, J. (1974). "A dendrite method for cluster analysis." Communications in Statistics-theory and Methods.
6. Han, J., Kamber, M., & Pei, J. (2012). "Data mining: concepts and techniques." Morgan Kaufmann Publishers. 